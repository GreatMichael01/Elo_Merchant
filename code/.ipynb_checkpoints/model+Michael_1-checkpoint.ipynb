{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss\n",
    "import lightgbm as lgb\n",
    "from sklearn import preprocessing\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the memory usage - Inspired by Panchajanya Banerjee\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n",
    "                start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple processing train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reduce_mem_usage(pd.read_csv('../data/train.csv', parse_dates=[\"first_active_month\"]))\n",
    "test = reduce_mem_usage(pd.read_csv('../data/test.csv', parse_dates=[\"first_active_month\"]))\n",
    "test[\"target\"] = -999\n",
    "data = pd.concat([train, test])\n",
    "data[\"year\"] = data[\"first_active_month\"].dt.year\n",
    "data[\"month\"] = data[\"first_active_month\"].dt.month\n",
    "data[\"day\"] = data[\"first_active_month\"].dt.day\n",
    "data[\"dayofyear\"] = data[\"first_active_month\"].dt.dayofyear\n",
    "data['week'] = data[\"first_active_month\"].dt.weekofyear\n",
    "data['dayofweek'] = data['first_active_month'].dt.dayofweek\n",
    "data['days'] = (datetime.date(2018, 2, 1) - data['first_active_month'].dt.date).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### project features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_trans(trans):  \n",
    "    \n",
    "    trans[\"authorized_flag\"] = trans[\"authorized_flag\"].map({\"Y\": 1, \"N\": 0})\n",
    "    trans[\"category_1\"] = trans[\"category_1\"].map({\"Y\": 1, \"N\":0})\n",
    "    trans[\"purchase_date\"] = pd.to_datetime(trans[\"purchase_date\"])\n",
    "    trans[\"year\"] = trans[\"purchase_date\"].dt.year\n",
    "    trans[\"month\"] = trans[\"purchase_date\"].dt.month\n",
    "    trans[\"day\"] = trans[\"purchase_date\"].dt.day\n",
    "    trans[\"weekofyear\"] = trans[\"purchase_date\"].dt.weekofyear\n",
    "    trans[\"dayofweek\"] = trans[\"purchase_date\"].dt.dayofweek\n",
    "    trans[\"weekend\"] = (trans[\"purchase_date\"].dt.weekday >= 5).astype(int)\n",
    "    trans[\"hour\"] = trans[\"purchase_date\"].dt.hour\n",
    "    trans[\"minute\"] = trans[\"purchase_date\"].dt.minute\n",
    "    trans[\"month_diff\"] = ((datetime.datetime.today() - trans[\"purchase_date\"]).dt.days) // 30\n",
    "    trans[\"month_diff\"] += trans[\"month_lag\"]\n",
    "    trans[\"category_2\"] = trans[\"category_2\"].fillna(2.0)\n",
    "    trans[\"category_3\"] = trans[\"category_3\"].fillna(\"A\")\n",
    "    trans[\"merchant_id\"] = trans[\"merchant_id\"].fillna(\"M_ID_00a6ca8a8a\")\n",
    "    agg_func = {\"mean\": [\"mean\"], \"sum\": [\"sum\"], \"max\": [\"max\"], \"min\": [\"min\"],\n",
    "                \"count\": [\"count\"], \"std\": [\"std\"], \"var\": [\"var\"]}\n",
    "    for col in [\"category_1\", \"category_2\", \"category_3\"]:\n",
    "        trans[col] = trans[\"purchase_amount\"].groupby(trans[col]).agg(agg_func)\n",
    "        trans[col] = trans[\"installments\"].groupby(trans[col]).agg(agg_func)\n",
    "        \n",
    "    # New Year: 1-1-2017\n",
    "    trans['New_year_2017'] = (pd.to_datetime('2017-01-01') - trans['purchase_date']).dt.days.apply(\n",
    "        lambda x: x if x > 0 and x < 60 else 0)  \n",
    "    # Martin Luther King Day: 1-16-2017\n",
    "    trans['Mlk_day_2017'] = (pd.to_datetime('2017-01-16') - trans['purchase_date']).dt.days.apply(\n",
    "        lambda x: x if x > 0 and x < 60 else 0) \n",
    "    # president day: 2-20-2017\n",
    "    trans['President_day_2017'] = (pd.to_datetime('2017-02-20') - trans['purchase_date']).dt.days.apply(\n",
    "        lambda x: x if x > 0 and x < 60 else 0)\n",
    "    # Valentine_Day: 2-14-2017\n",
    "    trans['Valentineâ€™s_day_2017'] = (pd.to_datetime('2017-02-14') - trans['purchase_date']).dt.days.apply(\n",
    "        lambda x: x if x > 0 and x < 60 else 0)\n",
    "    # Patrick day: 3-17-2017\n",
    "    trans['Patrick_day_2017'] = (pd.to_datetime('2017-03-17') - trans['purchase_date']).dt.days.apply(\n",
    "        lambda x: x if x > 0 and x < 60 else 0)\n",
    "    # Earth day: 4-5-2017\n",
    "    trans['Earth_day_2017'] = (pd.to_datetime('2017-04-24') - trans['purchase_date']).dt.days.apply(\n",
    "        lambda x: x if x > 0 and x < 60 else 0)\n",
    "    # president day: 4-5-2017\n",
    "    trans['Patrick_day_2017'] = (pd.to_datetime('2017-04-05') - trans['purchase_date']).dt.days.apply(\n",
    "        lambda x: x if x > 0 and x < 60 else 0)\n",
    "    # Mothers Day: 5-14-2017\n",
    "    trans['Mothers_day_2017'] = (pd.to_datetime('2017-05-14') - trans['purchase_date']).dt.days.apply(\n",
    "        lambda x: x if x > 0 and x < 60 else 0)\n",
    "    # Memorial Day: 5-29-2017\n",
    "    trans['Memorial_day_2017'] = (pd.to_datetime('2017-05-29') - trans['purchase_date']).dt.days.apply(\n",
    "        lambda x: x if x > 0 and x < 60 else 0)\n",
    "    # fathers Day: 6-18-2017\n",
    "    trans['Fathers_day_2017'] = (pd.to_datetime('2017-06-18') - trans['purchase_date']).dt.days.apply(\n",
    "        lambda x: x if x > 0 and x < 60 else 0)\n",
    "    # Independence Day: 7-4-2017\n",
    "    trans['Independence_day_2017'] = (pd.to_datetime('2017-07-04') - trans['purchase_date']).dt.days.apply(\n",
    "        lambda x: x if x > 0 and x < 60 else 0)\n",
    "    # Labor Day: 9-5-2017\n",
    "    trans['Labor_day_2017'] = (pd.to_datetime('2017-09-05') - trans['purchase_date']).dt.days.apply(\n",
    "        lambda x: x if x > 0 and x < 60 else 0)\n",
    "    # Columbus Day: 10-9-2017\n",
    "    trans['Columbus_day_2017'] = (pd.to_datetime('2017-10-09') - trans['purchase_date']).dt.days.apply(\n",
    "        lambda x: x if x > 0 and x < 60 else 0)\n",
    "    # Halloween's Day: 10-31-2017\n",
    "    trans['Halloweens_day_2017'] = (pd.to_datetime('2017-10-31') - trans['purchase_date']).dt.days.apply(\n",
    "        lambda x: x if x > 0 and x < 60 else 0)\n",
    "    # Global shopping Day: 11-11-2017\n",
    "    trans['Global_shopping_2017'] = (pd.to_datetime('2017-11-11') - trans['purchase_date']).dt.days.apply(\n",
    "        lambda x: x if x > 0 and x < 60 else 0)\n",
    "    # Global shopping Day: 11-23-2017\n",
    "    trans['Thanksgiving_Day_2017'] = (pd.to_datetime('2017-11-23') - trans['purchase_date']).dt.days.apply(\n",
    "        lambda x: x if x > 0 and x < 60 else 0)\n",
    "    # Christmas Eve Dayï¼š12-24-2017 \n",
    "    trans['Christmas_Eve_day_2017'] = (pd.to_datetime('2017-12-24') - trans['purchase_date']).dt.days.apply(\n",
    "        lambda x: x if x > 0 and x < 60 else 0)\n",
    "    # New_year: 2018-01-01\n",
    "    trans[\"New_year_2018\"] = (pd.to_datetime(\"2018-01-01\") - trans[\"purchase_date\"]).dt.days.apply(\n",
    "       lambda x: x if x > 0 and x < 60 else 0)\n",
    "    # Martin Luther King Day: 1-16-2018\n",
    "    trans['Mlk_day_2018'] = (pd.to_datetime('2018-01-16') - trans['purchase_date']).dt.days.apply(\n",
    "        lambda x: x if x > 0 and x < 60 else 0)\n",
    "    \n",
    "    return trans\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking Reference from Other Kernels\n",
    "def trans_agg(trans, prefix):\n",
    "    agg_func = {\"purchase_date\":[\"max\", \"min\", \"mean\", \"nunique\"],\n",
    "                \"month_diff\": [\"max\", \"min\", \"mean\", \"nunique\"],\n",
    "                \"weekend\": [\"sum\", \"mean\", \"nunique\"],\n",
    "                \"authorized_flag\": [\"max\", \"min\", \"sum\", \"mean\", \"nunique\"],\n",
    "                \"category_1\": [\"sum\", \"mean\", \"nunique\"],\n",
    "                \"installments\": [\"sum\", \"max\", \"min\", \"mean\", \"std\", \"median\", \"var\"],\n",
    "                \"purchase_amount\": [\"sum\", \"max\", \"min\", \"mean\", \"std\", \"median\", \"var\"],\n",
    "                \"month_lag\": [\"max\", \"min\", \"mean\", \"std\", \"var\"],\n",
    "                \"month_diff\": [\"max\", \"min\",\"mean\"],\n",
    "                \"card_id\": [\"size\"],\n",
    "                \"month\": [\"nunique\"],\n",
    "                \"hour\": [\"nunique\"],\n",
    "                \"weekofyear\": [\"nunique\"],\n",
    "                \"dayofweek\": [\"nunique\"],\n",
    "                \"year\": [\"nunique\"],\n",
    "                \"subsector_id\": [\"nunique\"],\n",
    "                \"merchant_category_id\": [\"nunique\"],\n",
    "                \"New_year_2017\": [\"mean\"],\n",
    "                \"Mlk_day_2017\": [\"mean\"],\n",
    "                \"President_day_2017\": [\"mean\"],\n",
    "                \"Valentineâ€™s_day_2017\": [\"mean\"],\n",
    "                \"Patrick_day_2017\": [\"mean\"],\n",
    "                \"Earth_day_2017\": [\"mean\"],\n",
    "                \"Patrick_day_2017\": [\"mean\"],\n",
    "                \"Mothers_day_2017\": [\"mean\"],\n",
    "                \"Memorial_day_2017\": [\"mean\"],\n",
    "                \"Fathers_day_2017\": [\"mean\"],\n",
    "                \"Independence_day_2017\": [\"mean\"],\n",
    "                \"Labor_day_2017\": [\"mean\"],\n",
    "                \"Columbus_day_2017\": [\"mean\"],\n",
    "                \"Halloweens_day_2017\": [\"mean\"],\n",
    "                \"Global_shopping_2017\": [\"mean\"],\n",
    "                \"Thanksgiving_Day_2017\": [\"mean\"],\n",
    "                \"Christmas_Eve_day_2017\": [\"mean\"],\n",
    "                \"New_year_2018\": [\"mean\"],\n",
    "                \"Mlk_day_2018\": [\"mean\"]}\n",
    "\n",
    "    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n",
    "    agg_trans.columns = [prefix + '_'.join(col).strip() for col in agg_trans.columns.values]\n",
    "    agg_trans.reset_index(inplace=True)\n",
    "    df = (trans.groupby('card_id').size().reset_index(name='{}transactions_count'.format(prefix)))\n",
    "    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n",
    "\n",
    "    return agg_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "def data_add_feat(data, prefix):\n",
    "    data[prefix + \"purchase_date_max\"] = pd.to_datetime(data[prefix + \"purchase_date_max\"])\n",
    "    data[prefix + \"purchase_date_min\"] = pd.to_datetime(data[prefix + \"purchase_date_min\"])\n",
    "    data[prefix + \"purchase_date_mean\"] = pd.to_datetime(data[prefix + \"purchase_date_mean\"])\n",
    "    data[prefix + \"purchase_date_diff1\"] = (data[prefix + \"purchase_date_max\"] - data[prefix + \"purchase_date_min\"]).dt.days\n",
    "    data[prefix + \"purchase_date_diff2\"] = (data[prefix + \"purchase_date_max\"] - data[prefix + \"purchase_date_mean\"]).dt.days\n",
    "    \n",
    "    data[prefix + \"purchase_date_average\"] = data[prefix + \"purchase_date_diff\"] / data[prefix + \"card_id_size\"]\n",
    "    data[prefix + \"purchase_date_uptonow\"] = (datetime.datetime.today() - data[prefix + \"purchase_date_max\"]).dt.days\n",
    "    data[prefix + \"first_buy\"] = (data[prefix + \"purchase_date_min\"] - data[\"first_active_month\"]).dt.days\n",
    "    data[prefix + \"last_buy\"] = (data[prefix + \"purchase_date_max\"] - data[\"first_active_month\"]).dt.days\n",
    "    for feature in [prefix + \"purchase_date_max\", prefix + \"purchase_date_min\"]:\n",
    "        data[feature] = data[feature].astype(np.int64) * 1e-9\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing historical transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_trans = reduce_mem_usage(pd.read_csv('../data/historical_transactions.csv'))\n",
    "hist_trans_pro = pro_trans(hist_trans)\n",
    "merge_trans = trans_agg(hist_trans_pro, prefix='hist_')\n",
    "# del hist_trans_pro\n",
    "gc.collect()\n",
    "\n",
    "data = pd.merge(data, merge_trans, on='card_id', how='left')\n",
    "del merge_trans\n",
    "gc.collect()\n",
    "\n",
    "data = data_add_feat(data, prefix=\"hist_\")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merch_card = pd.DataFrame()\n",
    "merch_card[\"card_id\"] = hist_trans.card_id\n",
    "merch_card[\"merchant_id\"] = hist_trans.merchant_id\n",
    "# merch_card = merch_card.drop_duplicates([\"card_id\"], keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merch_card = merch_card.drop_duplicates([\"card_id\"], keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, merch_card, on=\"card_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processing new_transacton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_trans = reduce_mem_usage(pd.read_csv('../data/new_merchant_transactions.csv'))\n",
    "new_trans = pro_trans(new_trans)\n",
    "merge_trans = trans_agg(new_trans, prefix='new_')\n",
    "# del new_trans\n",
    "gc.collect()\n",
    "\n",
    "data = pd.merge(data, merge_trans, on='card_id', how='left')\n",
    "del merge_trans\n",
    "gc.collect()\n",
    "\n",
    "data = data_add_feat(data, prefix=\"new_\")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merchant project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchants = pd.read_csv(\"../data/merchants.csv\")\n",
    "merchants[\"category_1\"] = merchants[\"category_1\"].map({\"Y\": 0, \"N\": 1})\n",
    "merchants[\"category_4\"] = merchants[\"category_4\"].map({\"Y\": 0, \"N\": 1})\n",
    "merchants[\"most_recent_sales_range\"] = merchants[\"most_recent_sales_range\"].map({\"A\":0,\"B\":1,\"C\":2,\"D\":3,\"E\":5})       \n",
    "merchants[\"most_recent_purchases_range\"] = merchants[\"most_recent_purchases_range\"].map({\"A\":0,\"B\":1,\"C\":2,\"D\":3,\"E\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchants_index = merchants[\"merchant_id\"].isin(data[\"merchant_id\"])\n",
    "merchants = merchants[merchants_index]\n",
    "merchants = merchants.drop_duplicates(\"merchant_id\")\n",
    "data = pd.merge(data, merchants, on=\"merchant_id\", how=\"left\")\n",
    "data = data.drop([\"merchant_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new feature â€”â€” new and hist cross features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added new feature - Interactive\n",
    "data['card_id_total'] = data['new_card_id_size'] + data['hist_card_id_size']\n",
    "data['purchase_amount_total'] = data['new_purchase_amount_sum'] + data['hist_purchase_amount_sum']\n",
    "data[\"purchase_amount_mean\"] = data[\"new_purchase_amount_mean\"] + data[\"hist_purchase_amount_mean\"]\n",
    "data[\"purchase_amount_max\"] = data[\"new_purchase_amount_max\"] + data[\"hist_purchase_amount_max\"]\n",
    "data[\"purchase_amount_min\"] = data[\"new_purchase_amount_min\"] + data[\"hist_purchase_amount_min\"]\n",
    "data[\"purchase_amount_std\"] = data[\"new_purchase_amount_std\"] + data[\"hist_purchase_amount_std\"]\n",
    "\n",
    "data[\"installments_total\"] = data[\"new_installments_sum\"] + data[\"hist_installments_sum\"]\n",
    "data[\"installments_mean\"] = data[\"new_installments_mean\"] + data[\"hist_installments_mean\"]\n",
    "data[\"installments_max\"] = data[\"new_installments_max\"] + data[\"hist_installments_max\"]\n",
    "data[\"installments_min\"] = data[\"new_installments_min\"] + data[\"hist_installments_min\"]\n",
    "data[\"installments_std\"] = data[\"new_installments_std\"] + data[\"hist_installments_std\"]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"hist_month_nunique_hist_month_diff_mean_add\"] = data[\"hist_month_nunique\"] + data[\"hist_month_diff_mean\"]   \n",
    "data[\"hist_month_nunique_hist_month_diff_mean_sub\"] = data[\"hist_month_nunique\"] - data[\"hist_month_diff_mean\"]   \n",
    "data[\"hist_month_nunique_hist_month_diff_mean_mul\"] = data[\"hist_month_nunique\"] * data[\"hist_month_diff_mean\"]\n",
    "data[\"hist_month_nunique_hist_month_diff_mean_div\"] = data[\"hist_month_nunique\"] / data[\"hist_month_diff_mean\"]\n",
    "\n",
    "data[\"hist_month_nunique_hist_authorized_flag_mean_add\"] = data[\"hist_month_nunique\"] + data[\"hist_authorized_flag_mean\"]    \n",
    "data[\"hist_month_nunique_hist_authorized_flag_mean_sub\"] = data[\"hist_month_nunique\"] - data[\"hist_authorized_flag_mean\"]\n",
    "data[\"hist_month_nunique_hist_authorized_flag_mean_mul\"] = data[\"hist_month_nunique\"] * data[\"hist_authorized_flag_mean\"]\n",
    "data[\"hist_month_nunique_hist_authorized_flag_mean_div\"] = data[\"hist_month_nunique\"] / data[\"hist_authorized_flag_mean\"]\n",
    "\n",
    "data[\"hist_month_diff_mean_hist_authorized_flag_mean_add\"] = data[\"hist_month_diff_mean\"] + data[\"hist_authorized_flag_mean\"]   \n",
    "data[\"hist_month_diff_mean_hist_authorized_flag_mean_sub\"] = data[\"hist_month_diff_mean\"] - data[\"hist_authorized_flag_mean\"]\n",
    "data[\"hist_month_diff_mean_hist_authorized_flag_mean_mul\"] = data[\"hist_month_diff_mean\"] * data[\"hist_authorized_flag_mean\"]\n",
    "data[\"hist_month_diff_mean_hist_authorized_flag_mean_div\"] = data[\"hist_month_diff_mean\"] / data[\"hist_authorized_flag_mean\"]\n",
    "\n",
    "data[\"hist_month_nunique_new_purchase_date_diff_add\"] = data[\"hist_month_nunique\"] + data[\"new_purchase_date_diff\"]\n",
    "data[\"hist_month_nunique_new_purchase_date_diff_sub\"] = data[\"hist_month_nunique\"] - data[\"new_purchase_date_diff\"]\n",
    "data[\"hist_month_nunique_new_purchase_date_diff_mul\"] = data[\"hist_month_nunique\"] * data[\"new_purchase_date_diff\"]\n",
    "data[\"hist_month_nunique_new_purchase_date_diff_div\"] = data[\"hist_month_nunique\"] / data[\"new_purchase_date_diff\"]\n",
    "\n",
    "data[\"hist_month_diff_mean_new_purchase_date_diff_add\"] = data[\"hist_month_diff_mean\"] + data[\"new_purchase_date_diff\"]\n",
    "data[\"hist_month_diff_mean_new_purchase_date_diff_sub\"] = data[\"hist_month_diff_mean\"] - data[\"new_purchase_date_diff\"]\n",
    "data[\"hist_month_diff_mean_new_purchase_date_diff_mul\"] = data[\"hist_month_diff_mean\"] * data[\"new_purchase_date_diff\"]\n",
    "data[\"hist_month_diff_mean_new_purchase_date_diff_div\"] = data[\"hist_month_diff_mean\"] / data[\"new_purchase_date_diff\"]\n",
    "\n",
    "data[\"hist_authorized_flag_mean_new_purchase_date_diff_add\"] = data[\"hist_authorized_flag_mean\"] + data[\"new_purchase_date_diff\"]    \n",
    "data[\"hist_authorized_flag_mean_new_purchase_date_diff_sub\"] = data[\"hist_authorized_flag_mean\"] - data[\"new_purchase_date_diff\"]\n",
    "data[\"hist_authorized_flag_mean_new_purchase_date_diff_mul\"] = data[\"hist_authorized_flag_mean\"] * data[\"new_purchase_date_diff\"]\n",
    "data[\"hist_authorized_flag_mean_new_purchase_date_diff_div\"] = data[\"hist_authorized_flag_mean\"] / data[\"new_purchase_date_diff\"]\n",
    "\n",
    "data[\"hist_month_nunique_hist_month_lag_mean_add\"] = data[\"hist_month_nunique\"] + data[\"hist_month_lag_mean\"]\n",
    "data[\"hist_month_nunique_hist_month_lag_mean_sub\"] = data[\"hist_month_nunique\"] - data[\"hist_month_lag_mean\"]\n",
    "data[\"hist_month_nunique_hist_month_lag_mean_mul\"] = data[\"hist_month_nunique\"] * data[\"hist_month_lag_mean\"]\n",
    "data[\"hist_month_nunique_hist_month_lag_mean_div\"] = data[\"hist_month_nunique\"] / data[\"hist_month_lag_mean\"]\n",
    "\n",
    "data[\"hist_month_diff_mean_hist_month_lag_mean_add\"] = data[\"hist_month_diff_mean\"] + data[\"hist_month_lag_mean\"]\n",
    "data[\"hist_month_diff_mean_hist_month_lag_mean_sub\"] = data[\"hist_month_diff_mean\"] - data[\"hist_month_lag_mean\"]\n",
    "data[\"hist_month_diff_mean_hist_month_lag_mean_mul\"] = data[\"hist_month_diff_mean\"] * data[\"hist_month_lag_mean\"]\n",
    "data[\"hist_month_diff_mean_hist_month_lag_mean_div\"] = data[\"hist_month_diff_mean\"] / data[\"hist_month_lag_mean\"]\n",
    "\n",
    "data[\"hist_authorized_flag_mean_hist_month_lag_mean_add\"] = data[\"hist_authorized_flag_mean\"] + data[\"hist_month_lag_mean\"]\n",
    "data[\"hist_authorized_flag_mean_hist_month_lag_mean_sub\"] = data[\"hist_authorized_flag_mean\"] - data[\"hist_month_lag_mean\"]\n",
    "data[\"hist_authorized_flag_mean_hist_month_lag_mean_mul\"] = data[\"hist_authorized_flag_mean\"] * data[\"hist_month_lag_mean\"]\n",
    "data[\"hist_authorized_flag_mean_hist_month_lag_mean_div\"] = data[\"hist_authorized_flag_mean\"] / data[\"hist_month_lag_mean\"]\n",
    "\n",
    "data[\"new_purchase_date_diff_hist_month_lag_mean_add\"] = data[\"new_purchase_date_diff\"] + data[\"hist_month_lag_mean\"]\n",
    "data[\"new_purchase_date_diff_hist_month_lag_mean_sub\"] = data[\"new_purchase_date_diff\"] - data[\"hist_month_lag_mean\"]\n",
    "data[\"new_purchase_date_diff_hist_month_lag_mean_mul\"] = data[\"new_purchase_date_diff\"] * data[\"hist_month_lag_mean\"]\n",
    "data[\"new_purchase_date_diff_hist_month_lag_mean_div\"] = data[\"new_purchase_date_diff\"] / data[\"hist_month_lag_mean\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for missing values in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in training set\n",
    "nulls = np.sum(data.isnull())\n",
    "nullcols = nulls.loc[(nulls != 0)]\n",
    "dtypes = data.dtypes\n",
    "dtypes2 = dtypes.loc[(nulls != 0)]\n",
    "info = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)\n",
    "\n",
    "numeric_dtypes = ['float64']\n",
    "numerics = []\n",
    "for i in data.columns:\n",
    "    if data[i].dtype in numeric_dtypes:\n",
    "        numerics.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data[\"target\"] != -999]\n",
    "test = data[data[\"target\"] == -999]\n",
    "test = test.drop([\"target\"], axis=1)\n",
    "\n",
    "train['outliers'] = 0\n",
    "train.loc[train['target'] < -30, 'outliers'] = 1\n",
    "train['outliers'].value_counts()\n",
    "\n",
    "for features in ['feature_1', 'feature_2', 'feature_3']:\n",
    "    order_label = train.groupby([features])['outliers'].mean()\n",
    "    train[features] = train[features].map(order_label)\n",
    "    test[features] = test[features].map(order_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining model with a model without outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train\n",
    "test_df = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part1: Training model without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df[\"outliers\"] == 0]\n",
    "target = train_df[\"target\"]\n",
    "del train_df[\"target\"]\n",
    "features = [c for c in train_df.columns if c not in [\"card_id\", \"first_active_month\", \"outliers\"]]\n",
    "categorical_feats = [c for c in features if \"feature_\" in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 32, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.001,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 8,\n",
    "         \"bagging_fraction\": 0.9,\n",
    "         \"bagging_seed\": 10,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"nthread\": 6}\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "oof = np.zeros(len(train_df))\n",
    "predictions = np.zeros(len(test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df,train_df['outliers'].values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval= -1,\n",
    "                    early_stopping_rounds = 200)\n",
    "    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,25))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../fig/lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_outliers = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\n",
    "model_without_outliers[\"target\"] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part2: Training model for outliers classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train\n",
    "test_df = test\n",
    "target = train_df[\"outliers\"]\n",
    "del train_df[\"outliers\"]\n",
    "del train_df[\"target\"]\n",
    "\n",
    "features = [c for c in train_df.columns if c not in [\"card_id\", \"first_active_month\"]]\n",
    "categorical_feats = [c for c in features if \"feature_\" in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'binary',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.001,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 10,\n",
    "         \"metric\": 'binary_logloss',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"nthread\": 6}\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "oof = np.zeros(len(train_df))\n",
    "predictions2 = np.zeros(len(test_df))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n",
    "    print(\"fold nÂ°{}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n",
    "    val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=-1, \n",
    "                    early_stopping_rounds = 200)\n",
    "    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions2 += clf.predict(test_df[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(log_loss(target, oof)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlier_prob = pd.DataFrame({\"card_id\": test_df[\"card_id\"].values})\n",
    "df_outlier_prob[\"target\"] = predictions2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: combining submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_id = pd.DataFrame(df_outlier_prob.sort_values(by=\"target\", ascending=False).head(25000)[\"card_id\"])\n",
    "best_submission = pd.read_csv(\"../submission/6911+6912+26121119.csv\")\n",
    "most_likely_liers = best_submission.merge(outlier_id, how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for card_id in most_likely_liers[\"card_id\"]:\n",
    "    model_without_outliers.loc[model_without_outliers[\"card_id\"] == card_id, \"target\"] = \\\n",
    "    most_likely_liers.loc[most_likely_liers[\"card_id\"] == card_id, \"target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_outliers.to_csv(\"../submission/\"+str(int(time.strftime(\"%Y%m%d%H%M%S\", time.localtime(time.time()))))+\".csv\", index=False)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
