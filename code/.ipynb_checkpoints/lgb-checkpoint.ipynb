{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('../data/new_merchant_transactions.csv',nrows=10)\n",
    "dtypes = sample.dtypes\n",
    "\n",
    "col_types = {}\n",
    "remap = {np.int64: np.int16, np.float64:np.float16}\n",
    "for column,typ in dtypes.items():\n",
    "    col_types[column] = remap.get(typ.type,typ.type)\n",
    "\n",
    "col_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transactions = pd.read_csv('../data/new_merchant_transactions.csv', parse_dates=['purchase_date'],dtype=col_types)\n",
    "historical_transactions = pd.read_csv('../data/historical_transactions.csv', parse_dates=['purchase_date'],dtype=col_types)\n",
    "\n",
    "def binarize(df):\n",
    "    for col in ['authorized_flag', 'category_1']:\n",
    "        df[col] = df[col].map({'Y':1, 'N':0}).astype(np.int8)\n",
    "    return df\n",
    "\n",
    "historical_transactions = binarize(historical_transactions)\n",
    "new_transactions = binarize(new_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_transactions.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transactions.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(input_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['elapsed_time'] = (datetime.date(2018, 2, 1) - df['first_active_month'].dt.date).dt.days\n",
    "    return df\n",
    "#_________________________________________\n",
    "train = read_data('../data/train.csv')\n",
    "test = read_data('../data/test.csv')\n",
    "\n",
    "target = train['target']\n",
    "del train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_transactions = pd.get_dummies(historical_transactions, columns=['category_2', 'category_3'])\n",
    "new_transactions = pd.get_dummies(new_transactions, columns=['category_2', 'category_3'])\n",
    "\n",
    "historical_transactions = reduce_mem_usage(historical_transactions)\n",
    "new_transactions = reduce_mem_usage(new_transactions)\n",
    "\n",
    "agg_fun = {'authorized_flag': ['sum', 'mean']}\n",
    "auth_mean = historical_transactions.groupby(['card_id']).agg(agg_fun)\n",
    "auth_mean.columns = ['_'.join(col).strip() for col in auth_mean.columns.values]\n",
    "auth_mean.reset_index(inplace=True)\n",
    "\n",
    "authorized_transactions = historical_transactions[historical_transactions['authorized_flag'] == 1]\n",
    "historical_transactions = historical_transactions[historical_transactions['authorized_flag'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_transactions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_transactions['purchase_month'] = historical_transactions['purchase_date'].dt.month\n",
    "authorized_transactions['purchase_month'] = authorized_transactions['purchase_date'].dt.month\n",
    "new_transactions['purchase_month'] = new_transactions['purchase_date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_transactions(history):\n",
    "    \n",
    "    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n",
    "                                      astype(np.int64) * 1e-9\n",
    "    \n",
    "    agg_func = {\n",
    "        'category_1': ['sum', 'mean'],\n",
    "        'category_2_1.0': ['mean'],\n",
    "        'category_2_2.0': ['mean'],\n",
    "        'category_2_3.0': ['mean'],\n",
    "        'category_2_4.0': ['mean'],\n",
    "        'category_2_5.0': ['mean'],\n",
    "        'category_3_A': ['mean'],\n",
    "        'category_3_B': ['mean'],\n",
    "        'category_3_C': ['mean'],\n",
    "        'merchant_id': ['nunique'],\n",
    "        'merchant_category_id': ['nunique'],\n",
    "        'state_id': ['nunique'],\n",
    "        'city_id': ['nunique'],\n",
    "        'subsector_id': ['nunique'],\n",
    "        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n",
    "        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n",
    "        'purchase_month': ['mean', 'max', 'min', 'std'],\n",
    "        'purchase_date': [np.ptp],\n",
    "        'month_lag': ['min', 'max']\n",
    "        }\n",
    "    \n",
    "    agg_history = history.groupby(['card_id']).agg(agg_func)\n",
    "    agg_history.columns = ['_'.join(col).strip() for col in agg_history.columns.values]\n",
    "    agg_history.reset_index(inplace=True)\n",
    "    \n",
    "    df = (history.groupby('card_id')\n",
    "          .size()\n",
    "          .reset_index(name='transactions_count'))\n",
    "    \n",
    "    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n",
    "    \n",
    "    return agg_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = aggregate_transactions(historical_transactions)\n",
    "history.columns = ['hist_' + c if c != 'card_id' else c for c in history.columns]\n",
    "history[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorized = aggregate_transactions(authorized_transactions)\n",
    "authorized.columns = ['auth_' + c if c != 'card_id' else c for c in authorized.columns]\n",
    "authorized[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = aggregate_transactions(new_transactions)\n",
    "new.columns = ['new_' + c if c != 'card_id' else c for c in new.columns]\n",
    "new[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_per_month(history):\n",
    "    grouped = history.groupby(['card_id', 'month_lag'])\n",
    "\n",
    "    agg_func = {\n",
    "            'purchase_amount': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n",
    "            'installments': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n",
    "            }\n",
    "\n",
    "    intermediate_group = grouped.agg(agg_func)\n",
    "    intermediate_group.columns = ['_'.join(col).strip() for col in intermediate_group.columns.values]\n",
    "    intermediate_group.reset_index(inplace=True)\n",
    "\n",
    "    final_group = intermediate_group.groupby('card_id').agg(['mean', 'std'])\n",
    "    final_group.columns = ['_'.join(col).strip() for col in final_group.columns.values]\n",
    "    final_group.reset_index(inplace=True)\n",
    "    \n",
    "    return final_group\n",
    "#___________________________________________________________\n",
    "final_group =  aggregate_per_month(historical_transactions) \n",
    "final_group[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train, history, on='card_id', how='left')\n",
    "test = pd.merge(test, history, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, authorized, on='card_id', how='left')\n",
    "test = pd.merge(test, authorized, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, new, on='card_id', how='left')\n",
    "test = pd.merge(test, new, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, final_group, on='card_id', how='left')\n",
    "test = pd.merge(test, final_group, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, auth_mean, on='card_id', how='left')\n",
    "test = pd.merge(test, auth_mean, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in train.columns if c not in ['card_id', 'first_active_month']]\n",
    "categorical_feats = [c for c in features if 'feature_' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'num_leaves': 100,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression',\n",
    "         'max_depth': 6,\n",
    "         'learning_rate': 0.005,\n",
    "         \"min_child_samples\": 20,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "oof = np.zeros(len(train))\n",
    "predictions = np.zeros(len(test))\n",
    "start = time.time()\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,25))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\n",
    "sub_df[\"target\"] = predictions\n",
    "sub_df.to_csv(\"../submission/submit.csv\", index=False)\n",
    "\n",
    "# online=3.721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lgb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "# Models\n",
    "from sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#Common data processors\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv',parse_dates=[\"first_active_month\"])\n",
    "test = pd.read_csv('../data/test.csv', parse_dates=[\"first_active_month\"])\n",
    "merchants = pd.read_csv('../data/merchants.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Now extract the month, year, day, weekday\n",
    "train[\"month\"] = train[\"first_active_month\"].dt.month\n",
    "train[\"year\"] = train[\"first_active_month\"].dt.year\n",
    "train['week'] = train[\"first_active_month\"].dt.weekofyear\n",
    "train['dayofweek'] = train['first_active_month'].dt.dayofweek\n",
    "train['days'] = (datetime.date(2018, 2, 1) - train['first_active_month'].dt.date).dt.days\n",
    "\n",
    "test[\"month\"] = test[\"first_active_month\"].dt.month\n",
    "test[\"year\"] = test[\"first_active_month\"].dt.year\n",
    "test['week'] = test[\"first_active_month\"].dt.weekofyear\n",
    "test['dayofweek'] = test['first_active_month'].dt.dayofweek\n",
    "test['days'] = (datetime.date(2018, 2, 1) - test['first_active_month'].dt.date).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking Reference from Other Kernels\n",
    "def aggregate_transactions(trans, prefix):  \n",
    "        \n",
    "    agg_func = {\n",
    "        'purchase_date' : ['max','min'],\n",
    "        'month_diff' : ['mean'],\n",
    "        'weekend' : ['sum', 'mean'],\n",
    "        'authorized_flag': ['sum', 'mean'],\n",
    "        'category_1': ['sum','mean'],\n",
    "        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n",
    "        'installments': ['sum', 'mean', 'max', 'min', 'std'],  \n",
    "        'merchant_id': ['nunique'],\n",
    "        'month_lag': ['max','min','mean','var'],\n",
    "        'month_diff' : ['mean'],\n",
    "        'card_id' : ['size'],\n",
    "        'month': ['nunique'],\n",
    "        'hour': ['nunique'],\n",
    "        'weekofyear': ['nunique'],\n",
    "        'dayofweek': ['nunique'],\n",
    "        'year': ['nunique'],\n",
    "        'subsector_id': ['nunique'],\n",
    "        'merchant_category_id' : ['nunique']\n",
    "    }\n",
    "    \n",
    "    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n",
    "    agg_trans.columns = [prefix + '_'.join(col).strip() \n",
    "                           for col in agg_trans.columns.values]\n",
    "    agg_trans.reset_index(inplace=True)\n",
    "    \n",
    "    df = (trans.groupby('card_id')\n",
    "          .size()\n",
    "          .reset_index(name='{}transactions_count'.format(prefix)))\n",
    "    \n",
    "    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n",
    "    \n",
    "    return agg_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_csv('../data/historical_transactions.csv')\n",
    "transactions['authorized_flag'] = transactions['authorized_flag'].map({'Y': 1, 'N': 0})\n",
    "transactions['category_1'] = transactions['category_1'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "transactions['purchase_date'] = pd.to_datetime(transactions['purchase_date'])\n",
    "transactions['year'] = transactions['purchase_date'].dt.year\n",
    "transactions['weekofyear'] = transactions['purchase_date'].dt.weekofyear\n",
    "transactions['month'] = transactions['purchase_date'].dt.month\n",
    "transactions['dayofweek'] = transactions['purchase_date'].dt.dayofweek\n",
    "transactions['weekend'] = (transactions.purchase_date.dt.weekday >=5).astype(int)\n",
    "transactions['hour'] = transactions['purchase_date'].dt.hour \n",
    "transactions['month_diff'] = ((datetime.datetime.today() - transactions['purchase_date']).dt.days)//30\n",
    "transactions['month_diff'] += transactions['month_lag']\n",
    "\n",
    "#impute missing values - This is now excluded.\n",
    "transactions['category_2'] = transactions['category_2'].fillna(1.0,inplace=True)\n",
    "transactions['category_3'] = transactions['category_3'].fillna('A',inplace=True)\n",
    "transactions['merchant_id'] = transactions['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_func = {'mean': ['mean']}\n",
    "for col in ['category_2','category_3']:\n",
    "    transactions[col+'_mean'] = transactions['purchase_amount'].groupby(transactions[col]).agg(agg_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "merge_trans = aggregate_transactions(transactions, prefix='hist_')\n",
    "del transactions\n",
    "gc.collect()\n",
    "\n",
    "train = pd.merge(train, merge_trans, on='card_id',how='left')\n",
    "test = pd.merge(test, merge_trans, on='card_id',how='left')\n",
    "del merge_trans\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "train['hist_purchase_date_max'] = pd.to_datetime(train['hist_purchase_date_max'])\n",
    "train['hist_purchase_date_min'] = pd.to_datetime(train['hist_purchase_date_min'])\n",
    "train['hist_purchase_date_diff'] = (train['hist_purchase_date_max'] - train['hist_purchase_date_min']).dt.days\n",
    "train['hist_purchase_date_average'] = train['hist_purchase_date_diff']/train['hist_card_id_size']\n",
    "train['hist_purchase_date_uptonow'] = (datetime.datetime.today() - train['hist_purchase_date_max']).dt.days\n",
    "train['hist_first_buy'] = (train['hist_purchase_date_min'] - train['first_active_month']).dt.days\n",
    "for feature in ['hist_purchase_date_max','hist_purchase_date_min']:\n",
    "    train[feature] = train[feature].astype(np.int64) * 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "test['hist_purchase_date_max'] = pd.to_datetime(test['hist_purchase_date_max'])\n",
    "test['hist_purchase_date_min'] = pd.to_datetime(test['hist_purchase_date_min'])\n",
    "test['hist_purchase_date_diff'] = (test['hist_purchase_date_max'] - test['hist_purchase_date_min']).dt.days\n",
    "test['hist_purchase_date_average'] = test['hist_purchase_date_diff']/test['hist_card_id_size']\n",
    "test['hist_purchase_date_uptonow'] = (datetime.datetime.today() - test['hist_purchase_date_max']).dt.days\n",
    "test['hist_first_buy'] = (test['hist_purchase_date_min'] - test['first_active_month']).dt.days\n",
    "for feature in ['hist_purchase_date_max','hist_purchase_date_min']:\n",
    "    test[feature] = test[feature].astype(np.int64) * 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now extract the data from the new transactions\n",
    "new_transactions = pd.read_csv('../data/new_merchant_transactions.csv')\n",
    "new_transactions['authorized_flag'] = new_transactions['authorized_flag'].map({'Y': 1, 'N': 0})\n",
    "new_transactions['category_1'] = new_transactions['category_1'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "new_transactions['purchase_date'] = pd.to_datetime(new_transactions['purchase_date'])\n",
    "new_transactions['year'] = new_transactions['purchase_date'].dt.year\n",
    "new_transactions['weekofyear'] = new_transactions['purchase_date'].dt.weekofyear\n",
    "new_transactions['month'] = new_transactions['purchase_date'].dt.month\n",
    "new_transactions['dayofweek'] = new_transactions['purchase_date'].dt.dayofweek\n",
    "new_transactions['weekend'] = (new_transactions.purchase_date.dt.weekday >=5).astype(int)\n",
    "new_transactions['hour'] = new_transactions['purchase_date'].dt.hour \n",
    "new_transactions['month_diff'] = ((datetime.datetime.today() - new_transactions['purchase_date']).dt.days)//30\n",
    "new_transactions['month_diff'] += new_transactions['month_lag']\n",
    "\n",
    "#impute missing values\n",
    "new_transactions['category_2'] = new_transactions['category_2'].fillna(1.0,inplace=True)\n",
    "new_transactions['category_3'] = new_transactions['category_3'].fillna('A',inplace=True)\n",
    "new_transactions['merchant_id'] = new_transactions['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "\n",
    "agg_func = {'mean': ['mean']}\n",
    "for col in ['category_2','category_3']:\n",
    "    new_transactions[col+'_mean'] = new_transactions['purchase_amount'].groupby(new_transactions[col]).agg(agg_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_new = aggregate_transactions(new_transactions, prefix='new_')\n",
    "del new_transactions\n",
    "gc.collect()\n",
    "\n",
    "train = pd.merge(train, merge_new, on='card_id',how='left')\n",
    "test = pd.merge(test, merge_new, on='card_id',how='left')\n",
    "del merge_new\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "train['new_purchase_date_max'] = pd.to_datetime(train['new_purchase_date_max'])\n",
    "train['new_purchase_date_min'] = pd.to_datetime(train['new_purchase_date_min'])\n",
    "train['new_purchase_date_diff'] = (train['new_purchase_date_max'] - train['new_purchase_date_min']).dt.days\n",
    "train['new_purchase_date_average'] = train['new_purchase_date_diff']/train['new_card_id_size']\n",
    "train['new_purchase_date_uptonow'] = (datetime.datetime.today() - train['new_purchase_date_max']).dt.days\n",
    "train['new_first_buy'] = (train['new_purchase_date_min'] - train['first_active_month']).dt.days\n",
    "for feature in ['new_purchase_date_max','new_purchase_date_min']:\n",
    "    train[feature] = train[feature].astype(np.int64) * 1e-9\n",
    "\n",
    "#Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "test['new_purchase_date_max'] = pd.to_datetime(test['new_purchase_date_max'])\n",
    "test['new_purchase_date_min'] = pd.to_datetime(test['new_purchase_date_min'])\n",
    "test['new_purchase_date_diff'] = (test['new_purchase_date_max'] - test['new_purchase_date_min']).dt.days\n",
    "test['new_purchase_date_average'] = test['new_purchase_date_diff']/test['new_card_id_size']\n",
    "test['new_purchase_date_uptonow'] = (datetime.datetime.today() - test['new_purchase_date_max']).dt.days\n",
    "test['new_first_buy'] = (test['new_purchase_date_min'] - test['first_active_month']).dt.days\n",
    "for feature in ['new_purchase_date_max','new_purchase_date_min']:\n",
    "    test[feature] = test[feature].astype(np.int64) * 1e-9\n",
    "    \n",
    "#added new feature - Interactive\n",
    "train['card_id_total'] = train['new_card_id_size'] + train['hist_card_id_size']\n",
    "train['purchase_amount_total'] = train['new_purchase_amount_sum'] + train['hist_purchase_amount_sum']\n",
    "\n",
    "test['card_id_total'] = test['new_card_id_size'] + test['hist_card_id_size']\n",
    "test['purchase_amount_total'] = test['new_purchase_amount_sum'] + test['hist_purchase_amount_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing values in training set\n",
    "nulls = np.sum(train.isnull())\n",
    "nullcols = nulls.loc[(nulls != 0)]\n",
    "dtypes = train.dtypes\n",
    "dtypes2 = dtypes.loc[(nulls != 0)]\n",
    "info = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing values in training set\n",
    "nulls = np.sum(test.isnull())\n",
    "nullcols = nulls.loc[(nulls != 0)]\n",
    "dtypes = test.dtypes\n",
    "dtypes2 = dtypes.loc[(nulls != 0)]\n",
    "info = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_dtypes = ['float64']\n",
    "numerics = []\n",
    "for i in train.columns:\n",
    "    if train[i].dtype in numeric_dtypes: \n",
    "        numerics.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_dtypes = ['float64']\n",
    "numerics = []\n",
    "for i in test.columns:\n",
    "    if test[i].dtype in numeric_dtypes: \n",
    "        numerics.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Outliers if any \n",
    "train['outliers'] = 0\n",
    "train.loc[train['target'] < -30, 'outliers'] = 1\n",
    "train['outliers'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features in ['feature_1','feature_2','feature_3']:\n",
    "    order_label = train.groupby([features])['outliers'].mean()\n",
    "    train[features] = train[features].map(order_label)\n",
    "    test[features] =  test[features].map(order_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X and Y\n",
    "df_train_columns = [c for c in train.columns if c not in ['card_id', 'first_active_month','target','outliers']]\n",
    "target = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.01,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"nthread\": 4,\n",
    "         \"random_state\": 4590}\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4590)\n",
    "oof = np.zeros(len(train))\n",
    "predictions = np.zeros(len(test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train,train['outliers'].values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][df_train_columns], label=target.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][df_train_columns], label=target.iloc[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=-1, early_stopping_rounds = 100)\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][df_train_columns], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = df_train_columns\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(test[df_train_columns], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "np.sqrt(mean_squared_error(oof, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "        .groupby(\"Feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,25))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"Feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "sample_submission['target'] = predictions\n",
    "sample_submission.to_csv('../submission/submission_ashish_lgbm12141011.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lgb3 online=3.695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transactions = pd.read_csv('../data/new_merchant_transactions.csv', parse_dates=['purchase_date'])\n",
    "historical_transactions = pd.read_csv('../data/historical_transactions.csv', parse_dates=['purchase_date'])\n",
    "\n",
    "def binarize(df):\n",
    "    for col in ['authorized_flag', 'category_1']:\n",
    "        df[col] = df[col].map({'Y':1, 'N':0})\n",
    "    return df\n",
    "\n",
    "historical_transactions = binarize(historical_transactions)\n",
    "new_transactions = binarize(new_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def read_data(input_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['elapsed_time'] = (datetime.date(2018, 2, 1) - df['first_active_month'].dt.date).dt.days\n",
    "    return df\n",
    "\n",
    "train = read_data('../data/train.csv')\n",
    "test = read_data('../data/test.csv')\n",
    "\n",
    "target = train['target']\n",
    "del train['target']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1304.89 Mb (54.8% reduction)\n",
      "Mem. usage decreased to 84.24 Mb (56.7% reduction)\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "historical_transactions = pd.get_dummies(historical_transactions, columns=['category_2', 'category_3'])\n",
    "new_transactions = pd.get_dummies(new_transactions, columns=['category_2', 'category_3'])\n",
    "\n",
    "historical_transactions = reduce_mem_usage(historical_transactions)\n",
    "new_transactions = reduce_mem_usage(new_transactions)\n",
    "\n",
    "agg_fun = {'authorized_flag': ['sum', 'mean']}\n",
    "auth_mean = historical_transactions.groupby(['card_id']).agg(agg_fun)\n",
    "auth_mean.columns = ['_'.join(col).strip() for col in auth_mean.columns.values]\n",
    "auth_mean.reset_index(inplace=True)\n",
    "\n",
    "authorized_transactions = historical_transactions[historical_transactions['authorized_flag'] == 1]\n",
    "historical_transactions = historical_transactions[historical_transactions['authorized_flag'] == 0]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "historical_transactions['purchase_month'] = historical_transactions['purchase_date'].dt.month\n",
    "authorized_transactions['purchase_month'] = authorized_transactions['purchase_date'].dt.month\n",
    "new_transactions['purchase_month'] = new_transactions['purchase_date'].dt.month\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def aggregate_transactions(history):\n",
    "    \n",
    "    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n",
    "                                      astype(np.int64) * 1e-9\n",
    "    \n",
    "    agg_func = {\n",
    "        'category_1': ['sum', 'mean'],\n",
    "        'category_2_1.0': ['mean'],\n",
    "        'category_2_2.0': ['mean'],\n",
    "        'category_2_3.0': ['mean'],\n",
    "        'category_2_4.0': ['mean'],\n",
    "        'category_2_5.0': ['mean'],\n",
    "        'category_3_A': ['mean'],\n",
    "        'category_3_B': ['mean'],\n",
    "        'category_3_C': ['mean'],\n",
    "        'merchant_id': ['nunique'],\n",
    "        'merchant_category_id': ['nunique'],\n",
    "        'state_id': ['nunique'],\n",
    "        'city_id': ['nunique'],\n",
    "        'subsector_id': ['nunique'],\n",
    "        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n",
    "        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n",
    "        'purchase_month': ['mean', 'max', 'min', 'std'],\n",
    "        'purchase_date': [np.ptp, 'min', 'max'],\n",
    "        'month_lag': ['min', 'max']\n",
    "        }\n",
    "    \n",
    "    agg_history = history.groupby(['card_id']).agg(agg_func)\n",
    "    agg_history.columns = ['_'.join(col).strip() for col in agg_history.columns.values]\n",
    "    agg_history.reset_index(inplace=True)\n",
    "    \n",
    "    df = (history.groupby('card_id')\n",
    "          .size()\n",
    "          .reset_index(name='transactions_count'))\n",
    "    \n",
    "    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n",
    "    \n",
    "    return agg_history\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1351: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(self.var(ddof=ddof, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = aggregate_transactions(historical_transactions)\n",
    "history.columns = ['hist_' + c if c != 'card_id' else c for c in history.columns]\n",
    "history[:2]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1351: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(self.var(ddof=ddof, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "authorized = aggregate_transactions(authorized_transactions)\n",
    "authorized.columns = ['auth_' + c if c != 'card_id' else c for c in authorized.columns]\n",
    "authorized[:2]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new = aggregate_transactions(new_transactions)\n",
    "new.columns = ['new_' + c if c != 'card_id' else c for c in new.columns]\n",
    "new[:2]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1351: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(self.var(ddof=ddof, **kwargs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def aggregate_per_month(history):\n",
    "    grouped = history.groupby(['card_id', 'month_lag'])\n",
    "\n",
    "    agg_func = {\n",
    "            'purchase_amount': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n",
    "            'installments': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n",
    "            }\n",
    "\n",
    "    intermediate_group = grouped.agg(agg_func)\n",
    "    intermediate_group.columns = ['_'.join(col).strip() for col in intermediate_group.columns.values]\n",
    "    intermediate_group.reset_index(inplace=True)\n",
    "\n",
    "    final_group = intermediate_group.groupby('card_id').agg(['mean', 'std'])\n",
    "    final_group.columns = ['_'.join(col).strip() for col in final_group.columns.values]\n",
    "    final_group.reset_index(inplace=True)\n",
    "    \n",
    "    return final_group\n",
    "#___________________________________________________________\n",
    "final_group =  aggregate_per_month(historical_transactions) \n",
    "final_group[:2]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (201917, 139)\n",
      "Test Shape: (123623, 139)\n",
      "Wall time: 5.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.merge(train, history, on='card_id', how='left')\n",
    "test = pd.merge(test, history, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, authorized, on='card_id', how='left')\n",
    "test = pd.merge(test, authorized, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, new, on='card_id', how='left')\n",
    "test = pd.merge(test, new, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, final_group, on='card_id', how='left')\n",
    "test = pd.merge(test, final_group, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, auth_mean, on='card_id', how='left')\n",
    "test = pd.merge(test, auth_mean, on='card_id', how='left')\n",
    "\n",
    "print(\"Train Shape:\", train.shape)\n",
    "print(\"Test Shape:\", test.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201917, 139)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in train.columns if c not in ['card_id', 'first_active_month']]\n",
    "categorical_feats = [c for c in features if 'feature_' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 32, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.005,\n",
    "         \"min_child_samples\": 20,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"nthread\": 4,\n",
    "         \"verbosity\": -1}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "oof = np.zeros(len(train))\n",
    "predictions = np.zeros(len(test))\n",
    "start = time.time()\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,25))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbparam = {'num_leaves': 31,\n",
    "            'boosting_type': 'rf',\n",
    "             'min_data_in_leaf': 30, \n",
    "             'objective':'regression',\n",
    "             'max_depth': -1,\n",
    "             'learning_rate': 0.01,\n",
    "             \"min_child_samples\": 20,\n",
    "             \"boosting\": \"gbdt\",\n",
    "             \"feature_fraction\": 0.9,\n",
    "             \"bagging_freq\": 1,\n",
    "             \"bagging_fraction\": 0.9 ,\n",
    "             \"bagging_seed\": 11,\n",
    "             \"metric\": 'rmse',\n",
    "             \"lambda_l1\": 0.1,\n",
    "             \"verbosity\": -1,\n",
    "             \"nthread\": 4,\n",
    "             \"random_state\": 4590}\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "folds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=4520)\n",
    "\n",
    "oof_lgb = np.zeros(len(train))\n",
    "predictions_lgb = np.zeros(len(test))\n",
    "start = time.time()\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(lgbparam, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 100)\n",
    "    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions_lgb += clf.predict(test[features], num_iteration=clf.best_iteration) / (5 * 2)\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof_lgb, target)**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,25))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\n",
    "sub_df[\"target\"] = predictions\n",
    "sub_df.to_csv(\"../submission/submit_lgb.csv\", index=False)\n",
    "\n",
    "sub_df1 = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\n",
    "sub_df1[\"target\"] = predictions_lgb\n",
    "sub_df1.to_csv(\"../submission/submit_lgb1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stack = np.vstack([oof,oof_lgb]).transpose()\n",
    "test_stack = np.vstack([predictions,predictions_lgb]).transpose()\n",
    "\n",
    "folds = RepeatedKFold(n_splits=5,n_repeats=1,random_state=4520)\n",
    "oof_stack = np.zeros(train_stack.shape[0])\n",
    "predictions_stack = np.zeros(test_stack.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, target)):\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack[val_idx], target.iloc[val_idx].values\n",
    "\n",
    "    print(\"-\" * 10 + \"Stacking \" + str(fold_) + \"-\" * 10)\n",
    "#     cb_model = CatBoostRegressor(iterations=3000, learning_rate=0.1, depth=8, l2_leaf_reg=20, bootstrap_type='Bernoulli',  eval_metric='RMSE', metric_period=50, od_type='Iter', od_wait=45, random_seed=17, allow_writing_files=False)\n",
    "#     cb_model.fit(trn_data, trn_y, eval_set=(val_data, val_y), cat_features=[], use_best_model=True, verbose=True)\n",
    "    clf = BayesianRidge()\n",
    "    clf.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack[val_idx] = clf.predict(val_data)\n",
    "    predictions_stack += clf.predict(test_stack) / 5\n",
    "\n",
    "\n",
    "np.sqrt(mean_squared_error(target.values, oof_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "sample_submission['target'] = predictions_stack\n",
    "sample_submission.to_csv('../submission/Bayesian_Ridge_Stacking.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "sample1 = pd.read_csv(\"../submission/submission_ashish_lgbm12141117.csv\")\n",
    "sample_submission['target'] = predictions * 0.5 + predictions_lgb * 0.5\n",
    "sample_submission.to_csv(\"../submission/Blend1.csv\", index = False)\n",
    "sample_submission['target'] = sample_submission['target'] * 0.3 + sample1['target'] * 0.7\n",
    "sample_submission.to_csv('../submission/Blend2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
