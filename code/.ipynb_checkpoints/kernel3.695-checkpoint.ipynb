{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1dee50e02ba20b8aabe9775e0f8b19c40e1bcff8"
   },
   "source": [
    "This is my revised kernel inspired by Chau Ngoc Huynh's kernel (3.699).\n",
    "\n",
    "**New Update : Application of Interaction on Categorical Variables followed by Stacking using Bayesian Ridge on Stratified K Folds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a594abe9f66c5a090177dff966d6bedf63718a9c"
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import gc\n",
    "gc.collect()\n",
    "import os\n",
    "from six.moves import urllib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "from scipy.stats import norm, skew\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bfbbf30dcb4c354827aae6fff1aeaa632a9e874e"
   },
   "outputs": [],
   "source": [
    "#Add All the Models Libraries\n",
    "\n",
    "# Scalers\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Models\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedKFold\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#Common data processors\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "798008f46ddb84ca5ec248990d56b64e5ec84e5a"
   },
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(123)\n",
    "gc.collect()\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47d5d544fa7e4bf9305814409c3cabc148b0169a"
   },
   "outputs": [],
   "source": [
    "#Reduce the memory usage - Inspired by Panchajanya Banerjee\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "450f05f1bb8bd470972ec8a8f80a96f4b56402a7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = reduce_mem_usage(pd.read_csv('../input/train.csv',parse_dates=[\"first_active_month\"]))\n",
    "test = reduce_mem_usage(pd.read_csv('../input/test.csv', parse_dates=[\"first_active_month\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2352513f1d53487e789e41fbfa050199154f8916"
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78407176c3c40b9d15227165028d6b61f94f3ba3"
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1858899e8d6a7a6808204567ded690e4a10810d7"
   },
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 2)\n",
    "sns.distplot(train.target, kde=True, fit = norm)\n",
    "plt.xlabel('Customer Loyality (Skewed)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce244a5a05abaa8c91ff3c73d8c5f86ba592c9a1"
   },
   "outputs": [],
   "source": [
    "# Now extract the month, year, day, weekday\n",
    "train[\"month\"] = train[\"first_active_month\"].dt.month\n",
    "train[\"year\"] = train[\"first_active_month\"].dt.year\n",
    "train['week'] = train[\"first_active_month\"].dt.weekofyear\n",
    "train['dayofweek'] = train['first_active_month'].dt.dayofweek\n",
    "train['days'] = (datetime.date(2018, 2, 1) - train['first_active_month'].dt.date).dt.days\n",
    "train['quarter'] = train['first_active_month'].dt.quarter\n",
    "train['is_month_start'] = train['first_active_month'].dt.is_month_start\n",
    "\n",
    "#Interaction Variables\n",
    "train['days_feature1'] = train['days'] * train['feature_1']\n",
    "train['days_feature2'] = train['days'] * train['feature_2']\n",
    "train['days_feature3'] = train['days'] * train['feature_3']\n",
    "\n",
    "test[\"month\"] = test[\"first_active_month\"].dt.month\n",
    "test[\"year\"] = test[\"first_active_month\"].dt.year\n",
    "test['week'] = test[\"first_active_month\"].dt.weekofyear\n",
    "test['dayofweek'] = test['first_active_month'].dt.dayofweek\n",
    "test['days'] = (datetime.date(2018, 2, 1) - test['first_active_month'].dt.date).dt.days\n",
    "test['quarter'] = test['first_active_month'].dt.quarter\n",
    "test['is_month_start'] = test['first_active_month'].dt.is_month_start\n",
    "\n",
    "#Interaction Variables\n",
    "test['days_feature1'] = test['days'] * train['feature_1']\n",
    "test['days_feature2'] = test['days'] * train['feature_2']\n",
    "test['days_feature3'] = test['days'] * train['feature_3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b54460353af274062f649afe26f034c4126a6a84"
   },
   "source": [
    "Now we will try to extract more features from Transactions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9835dd6582d3e9c36954b498810929c8438d2d2"
   },
   "outputs": [],
   "source": [
    "# Taking Reference from Other Kernels\n",
    "def aggregate_transaction_hist(trans, prefix):  \n",
    "        \n",
    "    agg_func = {\n",
    "        'purchase_date' : ['max','min'],\n",
    "        'month_diff' : ['mean', 'min', 'max', 'var'],\n",
    "        'weekend' : ['sum', 'mean'],\n",
    "        'authorized_flag': ['sum', 'mean'],\n",
    "        'category_1': ['sum','mean', 'max','min'],\n",
    "        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n",
    "        'installments': ['sum', 'mean', 'max', 'min', 'std'],  \n",
    "        'month_lag': ['max','min','mean','var'],\n",
    "        'card_id' : ['size'],\n",
    "        'month': ['nunique'],\n",
    "        'hour': ['nunique'],\n",
    "        'weekofyear': ['nunique'],\n",
    "        'dayofweek': ['nunique'],\n",
    "        'year': ['nunique'],\n",
    "        'subsector_id': ['nunique'],\n",
    "        'merchant_category_id' : ['nunique']\n",
    "    }\n",
    "    \n",
    "    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n",
    "    agg_trans.columns = [prefix + '_'.join(col).strip() \n",
    "                           for col in agg_trans.columns.values]\n",
    "    agg_trans.reset_index(inplace=True)\n",
    "    \n",
    "    df = (trans.groupby('card_id')\n",
    "          .size()\n",
    "          .reset_index(name='{}transactions_count'.format(prefix)))\n",
    "    \n",
    "    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n",
    "    \n",
    "    return agg_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "06a04f3393a83cb2687b76deba417012e18ef606",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transactions = reduce_mem_usage(pd.read_csv('../input/historical_transactions.csv'))\n",
    "transactions['authorized_flag'] = transactions['authorized_flag'].map({'Y': 1, 'N': 0})\n",
    "transactions['category_1'] = transactions['category_1'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1decf075c9422fe20823691972f055b8a1c84f50",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "transactions['purchase_date'] = pd.to_datetime(transactions['purchase_date'])\n",
    "transactions['year'] = transactions['purchase_date'].dt.year\n",
    "transactions['weekofyear'] = transactions['purchase_date'].dt.weekofyear\n",
    "transactions['month'] = transactions['purchase_date'].dt.month\n",
    "transactions['dayofweek'] = transactions['purchase_date'].dt.dayofweek\n",
    "transactions['weekend'] = (transactions.purchase_date.dt.weekday >=5).astype(int)\n",
    "transactions['hour'] = transactions['purchase_date'].dt.hour \n",
    "transactions['quarter'] = transactions['purchase_date'].dt.quarter\n",
    "transactions['is_month_start'] = transactions['purchase_date'].dt.is_month_start\n",
    "transactions['month_diff'] = ((datetime.datetime.today() - transactions['purchase_date']).dt.days)//30\n",
    "transactions['month_diff'] += transactions['month_lag']\n",
    "\n",
    "#impute missing values - This is now excluded.\n",
    "transactions['category_2'] = transactions['category_2'].fillna(1.0,inplace=True)\n",
    "transactions['category_3'] = transactions['category_3'].fillna('A',inplace=True)\n",
    "transactions['merchant_id'] = transactions['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "\n",
    "transactions['category_3'] = transactions['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "93a165f209a8853e4c8e1f624becd0506e6fb0f1"
   },
   "outputs": [],
   "source": [
    "agg_func = {\n",
    "        'mean': ['mean'],\n",
    "    }\n",
    "for col in ['category_2','category_3']:\n",
    "    transactions[col+'_mean'] = transactions['purchase_amount'].groupby(transactions[col]).agg('mean')\n",
    "    transactions[col+'_max'] = transactions['purchase_amount'].groupby(transactions[col]).agg('max')\n",
    "    transactions[col+'_min'] = transactions['purchase_amount'].groupby(transactions[col]).agg('min')\n",
    "    transactions[col+'_var'] = transactions['purchase_amount'].groupby(transactions[col]).agg('var')\n",
    "    agg_func[col+'_mean'] = ['mean']\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "95128e635d5c0ad26b170652971aca4fc9c41ad1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "merge_trans = aggregate_transaction_hist(transactions, prefix='hist_')\n",
    "del transactions\n",
    "gc.collect()\n",
    "train = pd.merge(train, merge_trans, on='card_id',how='left')\n",
    "test = pd.merge(test, merge_trans, on='card_id',how='left')\n",
    "del merge_trans\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c2ecbbf35630592774f720ccf88849cb8dfe9987"
   },
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a3e6c6e3dec6f2153d02a684fb7ddbbace4c5550"
   },
   "outputs": [],
   "source": [
    "#Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "train['hist_purchase_date_max'] = pd.to_datetime(train['hist_purchase_date_max'])\n",
    "train['hist_purchase_date_min'] = pd.to_datetime(train['hist_purchase_date_min'])\n",
    "train['hist_purchase_date_diff'] = (train['hist_purchase_date_max'] - train['hist_purchase_date_min']).dt.days\n",
    "train['hist_purchase_date_average'] = train['hist_purchase_date_diff']/train['hist_card_id_size']\n",
    "train['hist_purchase_date_uptonow'] = (datetime.datetime.today() - train['hist_purchase_date_max']).dt.days\n",
    "train['hist_purchase_date_uptomin'] = (datetime.datetime.today() - train['hist_purchase_date_min']).dt.days\n",
    "train['hist_first_buy'] = (train['hist_purchase_date_min'] - train['first_active_month']).dt.days\n",
    "for feature in ['hist_purchase_date_max','hist_purchase_date_min']:\n",
    "    train[feature] = train[feature].astype(np.int64) * 1e-9\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "370757bfba37b643f463f385d9e878ff27e320bc"
   },
   "outputs": [],
   "source": [
    "#Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "test['hist_purchase_date_max'] = pd.to_datetime(test['hist_purchase_date_max'])\n",
    "test['hist_purchase_date_min'] = pd.to_datetime(test['hist_purchase_date_min'])\n",
    "test['hist_purchase_date_diff'] = (test['hist_purchase_date_max'] - test['hist_purchase_date_min']).dt.days\n",
    "test['hist_purchase_date_average'] = test['hist_purchase_date_diff']/test['hist_card_id_size']\n",
    "test['hist_purchase_date_uptonow'] = (datetime.datetime.today() - test['hist_purchase_date_max']).dt.days\n",
    "test['hist_purchase_date_uptomin'] = (datetime.datetime.today() - test['hist_purchase_date_min']).dt.days\n",
    "\n",
    "test['hist_first_buy'] = (test['hist_purchase_date_min'] - test['first_active_month']).dt.days\n",
    "for feature in ['hist_purchase_date_max','hist_purchase_date_min']:\n",
    "    test[feature] = test[feature].astype(np.int64) * 1e-9\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "88381a0c6e50924860a133e7d61838c86ee1eab1"
   },
   "outputs": [],
   "source": [
    "# Taking Reference from Other Kernels\n",
    "def aggregate_transaction_new(trans, prefix):  \n",
    "        \n",
    "    agg_func = {\n",
    "        'purchase_date' : ['max','min'],\n",
    "        'month_diff' : ['mean', 'min', 'max', 'var'],\n",
    "        'weekend' : ['sum', 'mean'],\n",
    "        'authorized_flag': ['sum', 'mean'],\n",
    "        'category_1': ['sum','mean', 'max','min'],\n",
    "        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n",
    "        'installments': ['sum', 'mean', 'max', 'min', 'std'],  \n",
    "        'month_lag': ['max','min','mean','var'],\n",
    "        'card_id' : ['size'],\n",
    "        'month': ['nunique'],\n",
    "        'hour': ['nunique'],\n",
    "        'weekofyear': ['nunique'],\n",
    "        'dayofweek': ['nunique'],\n",
    "        'year': ['nunique'],\n",
    "        'subsector_id': ['nunique'],\n",
    "        'merchant_category_id' : ['nunique']\n",
    "    }\n",
    "    \n",
    "    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n",
    "    agg_trans.columns = [prefix + '_'.join(col).strip() \n",
    "                           for col in agg_trans.columns.values]\n",
    "    agg_trans.reset_index(inplace=True)\n",
    "    \n",
    "    df = (trans.groupby('card_id')\n",
    "          .size()\n",
    "          .reset_index(name='{}transactions_count'.format(prefix)))\n",
    "    \n",
    "    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n",
    "    \n",
    "    return agg_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9c653335214ef763a6d9e13de46db5800a75c5a3"
   },
   "outputs": [],
   "source": [
    "# Now extract the data from the new transactions\n",
    "new_transactions = reduce_mem_usage(pd.read_csv('../input/new_merchant_transactions.csv'))\n",
    "new_transactions['authorized_flag'] = new_transactions['authorized_flag'].map({'Y': 1, 'N': 0})\n",
    "new_transactions['category_1'] = new_transactions['category_1'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd7fc65f351e45fdd1e8a43b730b8ed50b2e4456",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "new_transactions['purchase_date'] = pd.to_datetime(new_transactions['purchase_date'])\n",
    "new_transactions['year'] = new_transactions['purchase_date'].dt.year\n",
    "new_transactions['weekofyear'] = new_transactions['purchase_date'].dt.weekofyear\n",
    "new_transactions['month'] = new_transactions['purchase_date'].dt.month\n",
    "new_transactions['dayofweek'] = new_transactions['purchase_date'].dt.dayofweek\n",
    "new_transactions['weekend'] = (new_transactions.purchase_date.dt.weekday >=5).astype(int)\n",
    "new_transactions['hour'] = new_transactions['purchase_date'].dt.hour \n",
    "new_transactions['quarter'] = new_transactions['purchase_date'].dt.quarter\n",
    "new_transactions['is_month_start'] = new_transactions['purchase_date'].dt.is_month_start\n",
    "new_transactions['month_diff'] = ((datetime.datetime.today() - new_transactions['purchase_date']).dt.days)//30\n",
    "new_transactions['month_diff'] += new_transactions['month_lag']\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "#impute missing values\n",
    "new_transactions['category_2'] = new_transactions['category_2'].fillna(1.0,inplace=True)\n",
    "new_transactions['category_3'] = new_transactions['category_3'].fillna('A',inplace=True)\n",
    "new_transactions['merchant_id'] = new_transactions['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "\n",
    "new_transactions['category_3'] = new_transactions['category_3'].map({'A':0, 'B':1, 'C':2}) \n",
    "\n",
    "aggs = {\n",
    "        'mean': ['mean'],\n",
    "    }\n",
    "\n",
    "for col in ['category_2','category_3']:\n",
    "    new_transactions[col+'_mean'] = new_transactions['purchase_amount'].groupby(new_transactions[col]).agg('mean')\n",
    "    new_transactions[col+'_max'] = new_transactions['purchase_amount'].groupby(new_transactions[col]).agg('max')\n",
    "    new_transactions[col+'_min'] = new_transactions['purchase_amount'].groupby(new_transactions[col]).agg('min')\n",
    "    new_transactions[col+'_var'] = new_transactions['purchase_amount'].groupby(new_transactions[col]).agg('var')\n",
    "    aggs[col+'_mean'] = ['mean']\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5ebf2d2ac4064c8e9a008f555c7f902ca0fdf853",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "merge_new = aggregate_transaction_new(new_transactions, prefix='new_')\n",
    "del new_transactions\n",
    "gc.collect()\n",
    "\n",
    "train = pd.merge(train, merge_new, on='card_id',how='left')\n",
    "test = pd.merge(test, merge_new, on='card_id',how='left')\n",
    "del merge_new\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4741d24d2c897ebd286807d0d3bd658800e316ca"
   },
   "outputs": [],
   "source": [
    "#Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "train['new_purchase_date_max'] = pd.to_datetime(train['new_purchase_date_max'])\n",
    "train['new_purchase_date_min'] = pd.to_datetime(train['new_purchase_date_min'])\n",
    "train['new_purchase_date_diff'] = (train['new_purchase_date_max'] - train['new_purchase_date_min']).dt.days\n",
    "train['new_purchase_date_average'] = train['new_purchase_date_diff']/train['new_card_id_size']\n",
    "train['new_purchase_date_uptonow'] = (datetime.datetime.today() - train['new_purchase_date_max']).dt.days\n",
    "train['new_purchase_date_uptomin'] = (datetime.datetime.today() - train['new_purchase_date_min']).dt.days\n",
    "train['new_first_buy'] = (train['new_purchase_date_min'] - train['first_active_month']).dt.days\n",
    "for feature in ['new_purchase_date_max','new_purchase_date_min']:\n",
    "    train[feature] = train[feature].astype(np.int64) * 1e-9\n",
    "\n",
    "#Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "test['new_purchase_date_max'] = pd.to_datetime(test['new_purchase_date_max'])\n",
    "test['new_purchase_date_min'] = pd.to_datetime(test['new_purchase_date_min'])\n",
    "test['new_purchase_date_diff'] = (test['new_purchase_date_max'] - test['new_purchase_date_min']).dt.days\n",
    "test['new_purchase_date_average'] = test['new_purchase_date_diff']/test['new_card_id_size']\n",
    "test['new_purchase_date_uptonow'] = (datetime.datetime.today() - test['new_purchase_date_max']).dt.days\n",
    "test['new_purchase_date_uptomin'] = (datetime.datetime.today() - test['new_purchase_date_min']).dt.days\n",
    "test['new_first_buy'] = (test['new_purchase_date_min'] - test['first_active_month']).dt.days\n",
    "for feature in ['new_purchase_date_max','new_purchase_date_min']:\n",
    "    test[feature] = test[feature].astype(np.int64) * 1e-9\n",
    "    \n",
    "#added new feature - Interactive\n",
    "train['card_id_total'] = train['new_card_id_size'] + train['hist_card_id_size']\n",
    "train['purchase_amount_total'] = train['new_purchase_amount_sum'] + train['hist_purchase_amount_sum']\n",
    "\n",
    "test['card_id_total'] = test['new_card_id_size'] + test['hist_card_id_size']\n",
    "test['purchase_amount_total'] = test['new_purchase_amount_sum'] + test['hist_purchase_amount_sum']\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de5dbc4519c349552d66f192c47d73872ddf0c71",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now check the shape of Train and Test Data\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9aa541a016e5f0c3c66d9eebd30e6c713711d6a"
   },
   "outputs": [],
   "source": [
    "train = train.drop(['card_id', 'first_active_month'], axis = 1)\n",
    "test = test.drop(['card_id', 'first_active_month'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b0f12459d09a2d88ea34a60c85c1aa7553ef1aa"
   },
   "source": [
    "Detect and Correct Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0dd1c98207ed527cfd85a6b1a71235f3424629ee"
   },
   "outputs": [],
   "source": [
    "# Remove the Outliers if any \n",
    "train['outliers'] = 0\n",
    "train.loc[train['target'] < -30, 'outliers'] = 1\n",
    "train['outliers'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b5d796a748bbdf483888c96f37634f07e4c8663"
   },
   "outputs": [],
   "source": [
    "for features in ['feature_1','feature_2','feature_3']:\n",
    "    order_label = train.groupby([features])['outliers'].mean()\n",
    "    train[features] = train[features].map(order_label)\n",
    "    test[features] =  test[features].map(order_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f82ea60a7f8d903a8e75bf75a833cb75ff5ac22"
   },
   "outputs": [],
   "source": [
    "# Get the X and Y\n",
    "df_train_columns = [c for c in train.columns if c not in ['target','outliers']] #features used for FFM\n",
    "cat_features = [c for c in df_train_columns if 'feature_' in c]  #categories used for FFM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "44a1a3d4999d2cd5ef8f35aaf1b98ef5d9b9c5b2"
   },
   "outputs": [],
   "source": [
    "target = train['target']\n",
    "del train['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dac5e3fdfb1731d5b883ab2268f386cf61a869f7"
   },
   "source": [
    "Apply Light GBM Modelling Technique with Stratified K Folds enumerated on training set and outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29d413b0cf270375808c3424410e71e07f71e15e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 27, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.015,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"nthread\": 4,\n",
    "         \"random_state\": 4950}\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4950)\n",
    "oof = np.zeros(len(train))\n",
    "predictions = np.zeros(len(test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train,train['outliers'].values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][df_train_columns], label=target.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][df_train_columns], label=target.iloc[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=-1, early_stopping_rounds = 200)\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][df_train_columns], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = df_train_columns\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(test[df_train_columns], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "np.sqrt(mean_squared_error(oof, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04426bf9e197bbef4aa7e13fa4dfeef14fad7fdc"
   },
   "source": [
    "Feature Importance - Stratified K Folds Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f594383f5fb83246203ce54b4892592673e699ce",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "        .groupby(\"Feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,25))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"Feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4e4259f0bc335d4060f6d28ddc044617c9c7f94d"
   },
   "outputs": [],
   "source": [
    "features = [c for c in train.columns if c not in ['card_id', 'first_active_month','target','outliers']]\n",
    "cat_features = [c for c in features if 'feature_' in c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4b3d96a1354b5f37e7ba1a845428948cf7f8d1f9"
   },
   "source": [
    "Apply Light GBM Modelling Technique with K Folds enumerated on training set and Target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "85f3d1c010115cd099839c8434fe37a9429d8696",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 27, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.015,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"nthread\": 4,\n",
    "         \"random_state\": 4950}\n",
    "\n",
    "folds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=4950)\n",
    "oof_2 = np.zeros(len(train))\n",
    "predictions_2 = np.zeros(len(test))\n",
    "feature_importance_df_2 = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=cat_features)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=cat_features)\n",
    "\n",
    "    num_round = 10000\n",
    "    clf_r = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=-1, early_stopping_rounds = 200)\n",
    "    oof_2[val_idx] = clf_r.predict(train.iloc[val_idx][features], num_iteration=clf_r.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf_r.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df_2 = pd.concat([feature_importance_df_2, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions_2 += clf_r.predict(test[features], num_iteration=clf_r.best_iteration) / (5 * 2)\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof_2, target)**0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e68611a4c9f1f1a042ddc5eedcfad33d89575b13"
   },
   "source": [
    "Feature Importance - Stratified K Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e3ecdfbeb0d615801156a32ff14eade7e994950",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = (feature_importance_df_2[[\"Feature\", \"importance\"]]\n",
    "        .groupby(\"Feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "best_features = feature_importance_df_2.loc[feature_importance_df_2.Feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,25))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"Feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d83f9093cd718a2e72b3d9b84a02a8e72156c625"
   },
   "source": [
    "Stacking the models together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a65e5bb380bc06a3dcac649cc1168457d3c156a3"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "train_stack = np.vstack([oof,oof_2]).transpose()\n",
    "test_stack = np.vstack([predictions, predictions_2]).transpose()\n",
    "\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=1, random_state=4590)\n",
    "oof_stack = np.zeros(train_stack.shape[0])\n",
    "predictions_3 = np.zeros(test_stack.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack[val_idx], target.iloc[val_idx].values\n",
    "    \n",
    "    clf_3 = BayesianRidge()\n",
    "    clf_3.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack[val_idx] = clf_3.predict(val_data)\n",
    "    predictions_3 += clf_3.predict(test_stack) / 5\n",
    "    \n",
    "np.sqrt(mean_squared_error(target.values, oof_stack))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8b3c5b718779a1d5729e89a5341258c44de24e2"
   },
   "source": [
    "Final Predictions and Submission File."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e87474495a3b14a96345eb2773fce5564268d9ac"
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../input/sample_submission.csv')\n",
    "sample_submission['target'] = predictions_3\n",
    "sample_submission.to_csv('submission_ashish_lgbm.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
