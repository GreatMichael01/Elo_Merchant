{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the memory usage - Inspired by Panchajanya Banerjee\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n",
    "                start_mem - end_mem) / start_mem))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  4.04 Mb (56.2% reduction)\n",
      "Mem. usage decreased to  2.24 Mb (52.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "train = reduce_mem_usage(pd.read_csv('../data/train.csv', parse_dates=[\"first_active_month\"]))\n",
    "test = reduce_mem_usage(pd.read_csv('../data/test.csv', parse_dates=[\"first_active_month\"]))\n",
    "\n",
    "# Now extract the month, year, day, weekday\n",
    "train[\"year\"] = train[\"first_active_month\"].dt.year\n",
    "train[\"month\"] = train[\"first_active_month\"].dt.month\n",
    "train[\"dayofyear\"] = train[\"first_active_month\"].dt.dayofyear\n",
    "train['week'] = train[\"first_active_month\"].dt.weekofyear\n",
    "train['dayofweek'] = train['first_active_month'].dt.dayofweek\n",
    "train['days'] = (datetime.date(2018, 2, 1) - train['first_active_month'].dt.date).dt.days\n",
    "\n",
    "test[\"year\"] = test[\"first_active_month\"].dt.year\n",
    "test[\"month\"] = test[\"first_active_month\"].dt.month\n",
    "test[\"dayofyear\"] = test[\"first_active_month\"].dt.dayofyear\n",
    "test['week'] = test[\"first_active_month\"].dt.weekofyear\n",
    "test['dayofweek'] = test['first_active_month'].dt.dayofweek\n",
    "test['days'] = (datetime.date(2018, 2, 1) - test['first_active_month'].dt.date).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Taking Reference from Other Kernels\n",
    "# def aggregate_transaction_hist(trans, prefix):\n",
    "#     agg_func = {\n",
    "#         'purchase_date': ['max', 'min'],\n",
    "#         'month_diff': ['mean'],\n",
    "#         'weekend': ['sum', 'mean'],\n",
    "#         'authorized_flag': ['sum', 'mean'],\n",
    "#         'category_1': ['sum', 'mean'],\n",
    "#         'purchase_amount': ['sum', 'mean', 'count', 'max', 'min', 'std'],\n",
    "#         'installments': ['sum', 'mean', 'count', 'max', 'min', 'std'],\n",
    "#         'merchant_id': ['nunique'],\n",
    "#         'month_lag': ['max', 'min', 'mean', 'var'],\n",
    "#         'month_diff': ['mean'],\n",
    "#         'card_id': ['size'],\n",
    "#         'month': ['nunique'],\n",
    "#         'hour': ['nunique'],\n",
    "#         'weekofyear': ['nunique'],\n",
    "#         'dayofweek': ['nunique'],\n",
    "#         'year': ['nunique'],\n",
    "#         'subsector_id': ['nunique'],\n",
    "#         'merchant_category_id': ['nunique'],\n",
    "#         'Christmas_Day_2017': ['mean'],\n",
    "#         'Mothers_Day_2017':['mean'],\n",
    "#         'fathers_day_2017': ['mean'],\n",
    "#         'Children_day_2017': ['mean'],\n",
    "#         'Black_Friday_2017': ['mean'],\n",
    "#         'Valentine_day_2017': ['mean'],\n",
    "#         'Mothers_Day_2018': ['mean']\n",
    "#     }\n",
    "\n",
    "#     agg_trans = trans.groupby(['card_id']).agg(agg_func)\n",
    "#     agg_trans.columns = [prefix + '_'.join(col).strip() for col in agg_trans.columns.values]\n",
    "#     agg_trans.reset_index(inplace=True)\n",
    "#     df = (trans.groupby('card_id').size().reset_index(name='{}transactions_count'.format(prefix)))\n",
    "#     agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n",
    "\n",
    "#     return agg_trans\n",
    "\n",
    "# Taking Reference from Other Kernels\n",
    "def aggregate_transaction_hist(trans, prefix):\n",
    "    agg_func = {'purchase_date': ['max', 'min'],\n",
    "                'month_diff': ['mean'],\n",
    "                'weekend': ['sum', 'mean'],\n",
    "                'authorized_flag': ['sum', 'mean'],\n",
    "                'category_1': ['sum', 'mean'],\n",
    "                'purchase_amount': ['sum', 'mean', 'count', 'max', 'min', 'std'],\n",
    "                'installments': ['sum', 'mean', 'count', 'max', 'min', 'std'],\n",
    "                'merchant_id': ['nunique'],\n",
    "                'month_lag': ['max', 'min', 'mean', 'var'],\n",
    "                'month_diff': ['mean'],\n",
    "                'card_id': ['size'],\n",
    "                'month': ['nunique'],\n",
    "                'hour': ['nunique'],\n",
    "                'weekofyear': ['nunique'],\n",
    "                'dayofweek': ['nunique'],\n",
    "                'year': ['nunique'],\n",
    "                'subsector_id': ['nunique'],\n",
    "                'merchant_category_id': ['nunique'],\n",
    "                'Christmas_Day_2017': ['mean'],\n",
    "                'Mothers_Day_2017':['mean'],\n",
    "                'fathers_day_2017': ['mean'],\n",
    "                'Children_day_2017': ['mean'],\n",
    "                'Black_Friday_2017': ['mean'],\n",
    "                'Valentine_day_2017': ['mean'],\n",
    "                'Mothers_Day_2018': ['mean']}\n",
    "    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n",
    "    agg_trans.columns = [prefix + '_'.join(col).strip() for col in agg_trans.columns.values]\n",
    "    agg_trans.reset_index(inplace=True)\n",
    "    df = (trans.groupby('card_id').size().reset_index(name='{}transactions_count'.format(prefix)))\n",
    "    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n",
    "\n",
    "    return agg_trans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1749.11 Mb (43.7% reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions = reduce_mem_usage(pd.read_csv('../data/historical_transactions.csv'))\n",
    "transactions['authorized_flag'] = transactions['authorized_flag'].map({'Y': 1, 'N': 0})\n",
    "transactions['category_1'] = transactions['category_1'].map({'Y': 1, 'N': 0})\n",
    "\n",
    "# Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "transactions['purchase_date'] = pd.to_datetime(transactions['purchase_date'])\n",
    "transactions['year'] = transactions['purchase_date'].dt.year\n",
    "transactions['weekofyear'] = transactions['purchase_date'].dt.weekofyear\n",
    "transactions['month'] = transactions['purchase_date'].dt.month\n",
    "transactions['dayofweek'] = transactions['purchase_date'].dt.dayofweek\n",
    "transactions['weekend'] = (transactions.purchase_date.dt.weekday >= 5).astype(int)\n",
    "transactions['hour'] = transactions['purchase_date'].dt.hour\n",
    "transactions['month_diff'] = ((datetime.datetime.today() - transactions['purchase_date']).dt.days) // 30\n",
    "transactions['month_diff'] += transactions['month_lag']\n",
    "\n",
    "# impute missing values - This is now excluded.\n",
    "transactions['category_2'] = transactions['category_2'].fillna(1.0, inplace=True)\n",
    "transactions['category_3'] = transactions['category_3'].fillna('A', inplace=True)\n",
    "transactions['merchant_id'] = transactions['merchant_id'].fillna('M_ID_00a6ca8a8a', inplace=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agg_func = {'mean': ['mean']}\n",
    "# for col in ['category_2', 'category_3']:\n",
    "#     transactions[col + '_mean'] = transactions['purchase_amount'].groupby(transactions[col]).agg(agg_func)\n",
    "\n",
    "# transactions['Christmas_Day_2017'] = (pd.to_datetime('2017-12-25') - transactions['purchase_date']).dt.days.apply(\n",
    "#     lambda x: x if x > 0 and x < 100 else 0)\n",
    "# # Mothers Day: May 14 2017\n",
    "# transactions['Mothers_Day_2017'] = (pd.to_datetime('2017-05-04') - transactions['purchase_date']).dt.days.apply(\n",
    "#     lambda x: x if x > 0 and x < 100 else 0)\n",
    "# # fathers day: August 13 2017\n",
    "# transactions['fathers_day_2017'] = (pd.to_datetime('2017-08-13') - transactions['purchase_date']).dt.days.apply(\n",
    "#     lambda x: x if x > 0 and x < 100 else 0)\n",
    "# # Childrens day: October 12 2017\n",
    "# transactions['Children_day_2017'] = (pd.to_datetime('2017-10-12') - transactions['purchase_date']).dt.days.apply(\n",
    "#     lambda x: x if x > 0 and x < 100 else 0)\n",
    "# # Black Friday : 24th November 2017\n",
    "# transactions['Black_Friday_2017'] = (pd.to_datetime('2017-11-24') - transactions['purchase_date']).dt.days.apply(\n",
    "#     lambda x: x if x > 0 and x < 100 else 0)\n",
    "# # Valentines Day\n",
    "# transactions['Valentine_day_2017'] = (pd.to_datetime('2017-06-12') - transactions['purchase_date']).dt.days.apply(\n",
    "#     lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "# # 2018\n",
    "# # Mothers Day: May 13 2018\n",
    "# transactions['Mothers_Day_2018'] = (pd.to_datetime('2018-05-13') - transactions['purchase_date']).dt.days.apply(\n",
    "#     lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# merge_trans = aggregate_transaction_hist(transactions, prefix='hist_')\n",
    "# del transactions\n",
    "# gc.collect()\n",
    "# train = pd.merge(train, merge_trans, on='card_id', how='left')\n",
    "# test = pd.merge(test, merge_trans, on='card_id', how='left')\n",
    "# del merge_trans\n",
    "# gc.collect()\n",
    "\n",
    "\n",
    "agg_func = {'mean': ['mean']}\n",
    "for col in ['category_2', 'category_3']:\n",
    "    transactions[col + '_mean'] = transactions['purchase_amount'].groupby(transactions[col]).agg(agg_func)\n",
    "\n",
    "# Christmas Eve\n",
    "transactions['Christmas_Eva_2017'] = (pd.to_datetime('2017-12-24') - transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)    \n",
    "# Christmas Day：12 25 2017 \n",
    "transactions['Christmas_Day_2017'] = (pd.to_datetime('2017-12-25') - transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# Mothers Day: May 14 2017\n",
    "transactions['Mothers_Day_2017'] = (pd.to_datetime('2017-05-04') - transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# fathers day: August 13 2017\n",
    "transactions['fathers_day_2017'] = (pd.to_datetime('2017-08-13') - transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# Childrens day: October 12 2017\n",
    "transactions['Children_day_2017'] = (pd.to_datetime('2017-10-12') - transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# Black Friday : 24th November 2017\n",
    "transactions['Black_Friday_2017'] = (pd.to_datetime('2017-11-24') - transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# Valentines Day\n",
    "transactions['Valentine_day_2017'] = (pd.to_datetime('2017-06-12') - transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# Global Shopping ：11-11\n",
    "transactions['Global_Shopping_2017'] = (pd.to_datetime('2017-11-11') - transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# New Year ：1-1-2018\n",
    "transactions['New_year_2017'] = (pd.to_datetime('2017-01-01') - transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# Independence_Day: 7-4-2017\n",
    "transactions['Independence_day_2017'] = (pd.to_datetime('2017-07-04') - transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# Valentine_Day: 2-14-2017\n",
    "transactions['Valentine’s_day_2017'] = (pd.to_datetime('2017-02-14') - transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# Halloween_Day: 10-30-2017\n",
    "transactions['Halloween_day_2017'] = (pd.to_datetime('2017-10-30') - transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# Mothers Day: May 13 2018\n",
    "transactions['Mothers_Day_2018'] = (pd.to_datetime('2018-05-13') - transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "merge_trans = aggregate_transaction_hist(transactions, prefix='hist_')\n",
    "del transactions\n",
    "gc.collect()\n",
    "# data = pd.merge(data, merge_trans, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, merge_trans, on='card_id', how='left')\n",
    "test = pd.merge(test, merge_trans, on='card_id', how='left')\n",
    "del merge_trans\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "train['hist_purchase_date_max'] = pd.to_datetime(train['hist_purchase_date_max'])\n",
    "train['hist_purchase_date_min'] = pd.to_datetime(train['hist_purchase_date_min'])\n",
    "train['hist_purchase_date_diff'] = (train['hist_purchase_date_max'] - train['hist_purchase_date_min']).dt.days\n",
    "train['hist_purchase_date_average'] = train['hist_purchase_date_diff'] / train['hist_card_id_size']\n",
    "train['hist_purchase_date_uptonow'] = (datetime.datetime.today() - train['hist_purchase_date_max']).dt.days\n",
    "train['hist_first_buy'] = (train['hist_purchase_date_min'] - train['first_active_month']).dt.days\n",
    "for feature in ['hist_purchase_date_max', 'hist_purchase_date_min']:\n",
    "    train[feature] = train[feature].astype(np.int64) * 1e-9\n",
    "\n",
    "# Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "test['hist_purchase_date_max'] = pd.to_datetime(test['hist_purchase_date_max'])\n",
    "test['hist_purchase_date_min'] = pd.to_datetime(test['hist_purchase_date_min'])\n",
    "test['hist_purchase_date_diff'] = (test['hist_purchase_date_max'] - test['hist_purchase_date_min']).dt.days\n",
    "test['hist_purchase_date_average'] = test['hist_purchase_date_diff'] / test['hist_card_id_size']\n",
    "test['hist_purchase_date_uptonow'] = (datetime.datetime.today() - test['hist_purchase_date_max']).dt.days\n",
    "test['hist_first_buy'] = (test['hist_purchase_date_min'] - test['first_active_month']).dt.days\n",
    "for feature in ['hist_purchase_date_max', 'hist_purchase_date_min']:\n",
    "    test[feature] = test[feature].astype(np.int64) * 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Taking Reference from Other Kernels\n",
    "# def aggregate_transaction_new(trans, prefix):\n",
    "#     agg_func = {\n",
    "#         'purchase_date': ['max', 'min'],\n",
    "#         'month_diff': ['mean'],\n",
    "#         'weekend': ['sum', 'mean'],\n",
    "#         'authorized_flag': ['sum'],\n",
    "#         'category_1': ['sum', 'mean'],\n",
    "#         'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n",
    "#         'installments': ['sum', 'mean', 'max', 'min', 'std'],\n",
    "#         'merchant_id': ['nunique'],\n",
    "#         'month_lag': ['max', 'min', 'mean', 'var'],\n",
    "#         'month_diff': ['mean'],\n",
    "#         'card_id': ['size'],\n",
    "#         'month': ['nunique'],\n",
    "#         'hour': ['nunique'],\n",
    "#         'weekofyear': ['nunique'],\n",
    "#         'dayofweek': ['nunique'],\n",
    "#         'year': ['nunique'],\n",
    "#         'subsector_id': ['nunique'],\n",
    "#         'merchant_category_id': ['nunique'],\n",
    "#         'Christmas_Day_2017': ['mean'],\n",
    "#         'Mothers_Day_2017':['mean'],\n",
    "#         'fathers_day_2017': ['mean'],\n",
    "#         'Children_day_2017': ['mean'],\n",
    "#         'Black_Friday_2017': ['mean'],\n",
    "#         'Valentine_Day_2017': ['mean'],\n",
    "#         'Mothers_Day_2018': ['mean']\n",
    "#     }\n",
    "\n",
    "#     agg_trans = trans.groupby(['card_id']).agg(agg_func)\n",
    "#     agg_trans.columns = [prefix + '_'.join(col).strip()\n",
    "#                          for col in agg_trans.columns.values]\n",
    "#     agg_trans.reset_index(inplace=True)\n",
    "\n",
    "#     df = (trans.groupby('card_id')\n",
    "#           .size()\n",
    "#           .reset_index(name='{}transactions_count'.format(prefix)))\n",
    "\n",
    "#     agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n",
    "\n",
    "#     return agg_trans\n",
    "\n",
    "\n",
    "# Taking Reference from Other Kernels\n",
    "def aggregate_transaction_new(trans, prefix):\n",
    "    agg_func = {\"purchase_date\":[\"max\", \"min\"],\n",
    "                \"month_diff\":[\"mean\"],\n",
    "                \"weekend\": [\"sum\", \"mean\"],\n",
    "                \"authorized_flag\": [\"sum\"],\n",
    "                \"category_1\": [\"sum\", \"mean\"],\n",
    "                \"purchase_amount\": [\"sum\", \"mean\", \"max\", \"min\", \"std\", \"var\", \"count\", \"median\"],\n",
    "                \"installments\": [\"sum\", \"mean\", \"max\", \"min\", \"std\", \"var\", \"count\", \"median\"],\n",
    "                \"merchant_id\": [\"nunique\"],\n",
    "                \"month_lag\": [\"max\", \"min\", \"mean\", \"var\", \"median\", \"sum\", \"count\", \"std\"],\n",
    "                \"month_diff\": [\"mean\"],\n",
    "                \"card_id\": [\"size\"],\n",
    "                \"month\": [\"nunique\"],\n",
    "                \"hour\": [\"nunique\"],\n",
    "                \"weekofyear\": [\"nunique\"],\n",
    "                \"dayofweek\": [\"nunique\"],\n",
    "                \"year\": [\"nunique\"],\n",
    "                \"subsector_id\": [\"nunique\"],\n",
    "                \"merchant_category_id\": [\"nunique\"],\n",
    "                \"Christmas_Day_2017\": [\"mean\"],\n",
    "                \"Mothers_Day_2017\": [\"mean\"],\n",
    "                \"fathers_day_2017\": [\"mean\"],\n",
    "                \"Children_day_2017\": [\"mean\"],\n",
    "                \"Black_Friday_2017\": [\"mean\"],\n",
    "                \"Valentine_Day_2017\": [\"mean\"],\n",
    "                \"Christmas_Eva_2017\": [\"mean\"],\n",
    "                \"Global_Shopping_2017\": [\"mean\"],\n",
    "                \"New_year_2017\": [\"mean\"],\n",
    "                \"Independence_day_2017\": [\"mean\"],\n",
    "                \"Valentine’s_day_2017\": [\"mean\"],\n",
    "                \"Halloween_day_2017\": [\"mean\"],\n",
    "                \"Mothers_Day_2018\": [\"mean\"]\n",
    "                }\n",
    "    agg_trans = trans.groupby([\"card_id\"]).agg(agg_func)\n",
    "    agg_trans.columns = [prefix + \"_\".join(col).strip() for col in agg_trans.columns.values]\n",
    "    agg_trans.reset_index(inplace=True)\n",
    "    df = (trans.groupby(\"card_id\").size().reset_index(name=\"{}transactions_count\".format(prefix)))\n",
    "    agg_trans = pd.merge(df, agg_trans, on=\"card_id\", how=\"left\")\n",
    "    \n",
    "    return agg_trans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 114.20 Mb (45.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Now extract the data from the new transactions\n",
    "new_transactions = reduce_mem_usage(pd.read_csv('../data/new_merchant_transactions.csv'))\n",
    "new_transactions['authorized_flag'] = new_transactions['authorized_flag'].map({'Y': 1, 'N': 0})\n",
    "new_transactions['category_1'] = new_transactions['category_1'].map({'Y': 1, 'N': 0})\n",
    "\n",
    "# Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "new_transactions['purchase_date'] = pd.to_datetime(new_transactions['purchase_date'])\n",
    "new_transactions['year'] = new_transactions['purchase_date'].dt.year\n",
    "new_transactions['weekofyear'] = new_transactions['purchase_date'].dt.weekofyear\n",
    "new_transactions['month'] = new_transactions['purchase_date'].dt.month\n",
    "new_transactions['dayofweek'] = new_transactions['purchase_date'].dt.dayofweek\n",
    "new_transactions['weekend'] = (new_transactions.purchase_date.dt.weekday >= 5).astype(int)\n",
    "new_transactions['hour'] = new_transactions['purchase_date'].dt.hour\n",
    "new_transactions['month_diff'] = ((datetime.datetime.today() - new_transactions['purchase_date']).dt.days) // 30\n",
    "new_transactions['month_diff'] += new_transactions['month_lag']\n",
    "\n",
    "# impute missing values\n",
    "new_transactions['category_2'] = new_transactions['category_2'].fillna(1.0, inplace=True)\n",
    "new_transactions['category_3'] = new_transactions['category_3'].fillna('A', inplace=True)\n",
    "new_transactions['merchant_id'] = new_transactions['merchant_id'].fillna('M_ID_00a6ca8a8a', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # New Features with Key Shopping times considered in the dataset. if the purchase has been made within 60 days, \n",
    "# # it is considered as an influence\n",
    "\n",
    "# # Christmas : December 25 2017\n",
    "# new_transactions['Christmas_Day_2017'] = (\n",
    "#             pd.to_datetime('2017-12-25') - new_transactions['purchase_date']).dt.days.apply(\n",
    "#     lambda x: x if x > 0 and x < 100 else 0)\n",
    "# # Mothers Day: May 14 2017 - Was not significant in Feature Importance\n",
    "# new_transactions['Mothers_Day_2017'] = (pd.to_datetime('2017-06-04') - new_transactions['purchase_date']).dt.days.apply(\n",
    "#     lambda x: x if x > 0 and x < 100 else 0)\n",
    "# # fathers day: August 13 2017\n",
    "# new_transactions['fathers_day_2017'] = (pd.to_datetime('2017-08-13') - new_transactions['purchase_date']).dt.days.apply(\n",
    "#     lambda x: x if x > 0 and x < 100 else 0)\n",
    "# # Childrens day: October 12 2017\n",
    "# new_transactions['Children_day_2017'] = (\n",
    "#             pd.to_datetime('2017-10-12') - new_transactions['purchase_date']).dt.days.apply(\n",
    "#     lambda x: x if x > 0 and x < 100 else 0)\n",
    "# # Valentine's Day : 12th June, 2017\n",
    "# new_transactions['Valentine_Day_2017'] = (\n",
    "#             pd.to_datetime('2017-06-12') - new_transactions['purchase_date']).dt.days.apply(\n",
    "#     lambda x: x if x > 0 and x < 100 else 0)\n",
    "# # Black Friday : 24th November 2017\n",
    "# new_transactions['Black_Friday_2017'] = (\n",
    "#             pd.to_datetime('2017-11-24') - new_transactions['purchase_date']).dt.days.apply(\n",
    "#     lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "# # 2018\n",
    "# # Mothers Day: May 13 2018\n",
    "# new_transactions['Mothers_Day_2018'] = (pd.to_datetime('2018-05-13') - new_transactions['purchase_date']).dt.days.apply(\n",
    "#     lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "# agg_func = {'mean': ['mean']}\n",
    "# for col in ['category_2', 'category_3']:\n",
    "#     new_transactions[col + '_mean'] = new_transactions['purchase_amount'].groupby(new_transactions[col]).agg(agg_func)\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# merge_new = aggregate_transaction_new(new_transactions, prefix='new_')\n",
    "# del new_transactions\n",
    "# gc.collect()\n",
    "\n",
    "# train = pd.merge(train, merge_new, on='card_id', how='left')\n",
    "# test = pd.merge(test, merge_new, on='card_id', how='left')\n",
    "# del merge_new\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "\n",
    "# New Features with Key Shopping times considered in the dataset. if the purchase has been made within 60 days, \n",
    "# it is considered as an influence\n",
    "agg_func = {'mean': ['mean']}\n",
    "for col in ['category_2', 'category_3']:\n",
    "    new_transactions[col + '_mean'] = new_transactions['purchase_amount'].groupby(new_transactions[col]).agg(agg_func)\n",
    "\n",
    "# Christmas Eve\n",
    "new_transactions['Christmas_Eva_2017'] = (pd.to_datetime('2017-12-24') - new_transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)    \n",
    "# Christmas Day：12 25 2017 \n",
    "new_transactions['Christmas_Day_2017'] = (pd.to_datetime('2017-12-25') - new_transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# Mothers Day: May 14 2017\n",
    "new_transactions['Mothers_Day_2017'] = (pd.to_datetime('2017-05-04') - new_transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# fathers day: August 13 2017\n",
    "new_transactions['fathers_day_2017'] = (pd.to_datetime('2017-08-13') - new_transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# Childrens day: October 12 2017\n",
    "new_transactions['Children_day_2017'] = (pd.to_datetime('2017-10-12') - new_transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# Black Friday : 24th November 2017\n",
    "new_transactions['Black_Friday_2017'] = (pd.to_datetime('2017-11-24') - new_transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# Valentines Day\n",
    "new_transactions['Valentine_Day_2017'] = (pd.to_datetime('2017-06-12') - new_transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# Global Shopping ：11-11\n",
    "new_transactions['Global_Shopping_2017'] = (pd.to_datetime('2017-11-11') - new_transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# New Year ：1-1-2018\n",
    "new_transactions['New_year_2017'] = (pd.to_datetime('2017-01-01') - new_transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# Independence_Day: 7-4-2017\n",
    "new_transactions['Independence_day_2017'] = (pd.to_datetime('2017-07-04') - new_transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# Valentine_Day: 2-14-2017\n",
    "new_transactions['Valentine’s_day_2017'] = (pd.to_datetime('2017-02-14') - new_transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# Halloween_Day: 10-30-2017\n",
    "new_transactions['Halloween_day_2017'] = (pd.to_datetime('2017-10-30') - new_transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "# Mothers Day: May 13 2018\n",
    "new_transactions['Mothers_Day_2018'] = (pd.to_datetime('2018-05-13') - new_transactions['purchase_date']).dt.days.apply(\n",
    "    lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "merge_new = aggregate_transaction_new(new_transactions, prefix='new_')\n",
    "del new_transactions\n",
    "gc.collect()\n",
    "# data = pd.merge(data, merge_new, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, merge_new, on='card_id', how='left')\n",
    "test = pd.merge(test, merge_new, on='card_id', how='left')\n",
    "del merge_new\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "train['new_purchase_date_max'] = pd.to_datetime(train['new_purchase_date_max'])\n",
    "train['new_purchase_date_min'] = pd.to_datetime(train['new_purchase_date_min'])\n",
    "train['new_purchase_date_diff'] = (train['new_purchase_date_max'] - train['new_purchase_date_min']).dt.days\n",
    "train['new_purchase_date_average'] = train['new_purchase_date_diff'] / train['new_card_id_size']\n",
    "train['new_purchase_date_uptonow'] = (datetime.datetime.today() - train['new_purchase_date_max']).dt.days\n",
    "train['new_first_buy'] = (train['new_purchase_date_min'] - train['first_active_month']).dt.days\n",
    "for feature in ['new_purchase_date_max', 'new_purchase_date_min']:\n",
    "    train[feature] = train[feature].astype(np.int64) * 1e-9\n",
    "\n",
    "# Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "test['new_purchase_date_max'] = pd.to_datetime(test['new_purchase_date_max'])\n",
    "test['new_purchase_date_min'] = pd.to_datetime(test['new_purchase_date_min'])\n",
    "test['new_purchase_date_diff'] = (test['new_purchase_date_max'] - test['new_purchase_date_min']).dt.days\n",
    "test['new_purchase_date_average'] = test['new_purchase_date_diff'] / test['new_card_id_size']\n",
    "test['new_purchase_date_uptonow'] = (datetime.datetime.today() - test['new_purchase_date_max']).dt.days\n",
    "test['new_first_buy'] = (test['new_purchase_date_min'] - test['first_active_month']).dt.days\n",
    "for feature in ['new_purchase_date_max', 'new_purchase_date_min']:\n",
    "    test[feature] = test[feature].astype(np.int64) * 1e-9\n",
    "\n",
    "# added new feature - Interactive\n",
    "train['card_id_total'] = train['new_card_id_size'] + train['hist_card_id_size']\n",
    "train['purchase_amount_total'] = train['new_purchase_amount_sum'] + train['hist_purchase_amount_sum']\n",
    "\n",
    "test['card_id_total'] = test['new_card_id_size'] + test['hist_card_id_size']\n",
    "test['purchase_amount_total'] = test['new_purchase_amount_sum'] + test['hist_purchase_amount_sum']\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in training set\n",
    "nulls = np.sum(train.isnull())\n",
    "nullcols = nulls.loc[(nulls != 0)]\n",
    "dtypes = train.dtypes\n",
    "dtypes2 = dtypes.loc[(nulls != 0)]\n",
    "info = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)\n",
    "\n",
    "# Check for missing values in training set\n",
    "nulls = np.sum(test.isnull())\n",
    "nullcols = nulls.loc[(nulls != 0)]\n",
    "dtypes = test.dtypes\n",
    "dtypes2 = dtypes.loc[(nulls != 0)]\n",
    "info = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)\n",
    "\n",
    "numeric_dtypes = ['float64']\n",
    "numerics = []\n",
    "for i in train.columns:\n",
    "    if train[i].dtype in numeric_dtypes:\n",
    "        numerics.append(i)\n",
    "\n",
    "numeric_dtypes = ['float64']\n",
    "numerics = []\n",
    "for i in test.columns:\n",
    "    if test[i].dtype in numeric_dtypes:\n",
    "        numerics.append(i)\n",
    "\n",
    "train['outliers'] = 0\n",
    "train.loc[train['target'] < -30, 'outliers'] = 1\n",
    "train['outliers'].value_counts()\n",
    "\n",
    "for features in ['feature_1', 'feature_2', 'feature_3']:\n",
    "    order_label = train.groupby([features])['outliers'].mean()\n",
    "    train[features] = train[features].map(order_label)\n",
    "    test[features] = test[features].map(order_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9571]\ttraining's rmse: 3.37726\tvalid_1's rmse: 3.66105\n",
      "fold 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[8500]\ttraining's rmse: 3.39652\tvalid_1's rmse: 3.66607\n",
      "fold 2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[10175]\ttraining's rmse: 3.37314\tvalid_1's rmse: 3.64758\n",
      "fold 3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[10580]\ttraining's rmse: 3.35826\tvalid_1's rmse: 3.65056\n",
      "fold 4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[10519]\ttraining's rmse: 3.36647\tvalid_1's rmse: 3.65551\n"
     ]
    }
   ],
   "source": [
    "df_train_columns = [c for c in train.columns if c not in ['card_id', 'first_active_month', 'target', 'outliers']]\n",
    "target = train['target']\n",
    "\n",
    "param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 32, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.001,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"nthread\": -1}\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "oof = np.zeros(len(train))\n",
    "predictions = np.zeros(len(test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['outliers'].values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][df_train_columns], label=target.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][df_train_columns], label=target.iloc[val_idx])\n",
    "\n",
    "    num_round = 20000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets=[trn_data, val_data], verbose_eval=-1,\n",
    "                    early_stopping_rounds=100)\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][df_train_columns], num_iteration=clf.best_iteration)\n",
    "\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = df_train_columns\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "    predictions += clf.predict(test[df_train_columns], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "np.sqrt(mean_squared_error(oof, target))\n",
    "\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "sample_submission['target'] = predictions\n",
    "sample_submission.to_csv('../submission/submission_lgb20000fujia.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
