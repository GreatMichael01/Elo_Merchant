{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"../data_feat/train_df.csv\"\n",
    "TEST_FILE = \"../data_feat/test_df.csv\"\n",
    "SUB_FILE = \"../submission/\"\n",
    "NUM_SPLITS = 2\n",
    "RANDOM_SEED = 2018\n",
    "ALL_COLS = pd.read_csv(TRAIN_FILE).columns\n",
    "CATEGORICAL_COLS = [\"feature_1\", \"feature_2\", \"feature_3\", 'hist_category_1_max',\n",
    "                    'hist_category_1_min', 'hist_category_1_mean', 'hist_category_2_max',\n",
    "                    'hist_category_2_min', 'hist_category_2_mean', 'hist_category_3_max',\n",
    "                    'hist_category_3_min', 'hist_category_3_mean', 'hist_merchant_id_nunique',\n",
    "                    'hist_card_id_size', 'hist_card_id_nunique', 'hist_subsector_id_max',\n",
    "                    'hist_subsector_id_min', 'hist_subsector_id_nunique', 'hist_merchant_category_id_max',\n",
    "                    'hist_merchant_category_id_min', 'hist_merchant_category_id_nunique',\n",
    "                    'new_category_1_max', 'new_category_1_min', 'new_category_1_mean',\n",
    "                    'new_category_2_max', 'new_category_2_min', 'new_category_2_mean',\n",
    "                    'new_category_3_max', 'new_category_3_min', 'new_category_3_mean',\n",
    "                    'new_merchant_id_nunique', 'new_card_id_size', 'new_card_id_nunique',\n",
    "                    'new_subsector_id_max', 'new_subsector_id_min', 'new_subsector_id_nunique',\n",
    "                    'new_merchant_category_id_max', 'new_merchant_category_id_min',\n",
    "                    'new_merchant_category_id_nunique', 'merchant_group_id', 'merchant_category_id',\n",
    "                    'subsector_id', 'category_1', 'category_4', 'city_id', 'state_id', 'category_2',\n",
    "                    'card_id_total']\n",
    "NUMERIC_COLS = ALL_COLS.drop(CATEGORICAL_COLS)\n",
    "IGNORE_COLS = [\"card_id\", \"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDictionary(object):\n",
    "    def __init__(self,trainfile=None,testfile=None,\n",
    "                 dfTrain=None,dfTest=None,numeric_cols=[],\n",
    "                 ignore_cols=[]):\n",
    "        assert not ((trainfile is None) and (dfTrain is None)), \"trainfile or dfTrain at least one is set\"\n",
    "        assert not ((trainfile is not None) and (dfTrain is not None)), \"only one can be set\"\n",
    "        assert not ((testfile is None) and (dfTest is None)), \"testfile or dfTest at least one is set\"\n",
    "        assert not ((testfile is not None) and (dfTest is not None)), \"only one can be set\"\n",
    "\n",
    "        self.trainfile = trainfile\n",
    "        self.testfile = testfile\n",
    "        self.dfTrain = dfTrain\n",
    "        self.dfTest = dfTest\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.ignore_cols = ignore_cols\n",
    "        self.gen_feat_dict()\n",
    "\n",
    "    def gen_feat_dict(self):\n",
    "        if self.dfTrain is None:\n",
    "            dfTrain = pd.read_csv(self.trainfile)\n",
    "\n",
    "        else:\n",
    "            dfTrain = self.dfTrain\n",
    "\n",
    "        if self.dfTest is None:\n",
    "            dfTest = pd.read_csv(self.testfile)\n",
    "\n",
    "        else:\n",
    "            dfTest = self.dfTest\n",
    "\n",
    "        df = pd.concat([dfTrain,dfTest])\n",
    "\n",
    "        self.feat_dict = {}\n",
    "        tc = 0\n",
    "        for col in df.columns:\n",
    "            if col in self.ignore_cols:\n",
    "                continue\n",
    "            if col in self.numeric_cols:\n",
    "                self.feat_dict[col] = tc\n",
    "                tc += 1\n",
    "\n",
    "            else:\n",
    "                us = df[col].unique()\n",
    "                print(us)\n",
    "                self.feat_dict[col] = dict(zip(us,range(tc,len(us)+tc)))\n",
    "                tc += len(us)\n",
    "\n",
    "        self.feat_dim = tc\n",
    "\n",
    "\n",
    "class DataParser(object):\n",
    "    def __init__(self,feat_dict):\n",
    "        self.feat_dict = feat_dict\n",
    "\n",
    "    def parse(self,infile=None,df=None,has_label=False):\n",
    "        assert not ((infile is None) and (df is None)), \"infile or df at least one is set\"\n",
    "        assert not ((infile is not None) and (df is not None)), \"only one can be set\"\n",
    "\n",
    "\n",
    "        if infile is None:\n",
    "            dfi = df.copy()\n",
    "        else:\n",
    "            dfi = pd.read_csv(infile)\n",
    "\n",
    "        if has_label:\n",
    "            y = dfi['target'].values.tolist()\n",
    "            dfi.drop(['card_id','target'],axis=1,inplace=True)\n",
    "        else:\n",
    "            ids = dfi['card_id'].values.tolist()\n",
    "            dfi.drop(['card_id'],axis=1,inplace=True)\n",
    "        # dfi for feature index\n",
    "        # dfv for feature value which can be either binary (1/0) or float (e.g., 10.24)\n",
    "        dfv = dfi.copy()\n",
    "        for col in dfi.columns:\n",
    "            if col in self.feat_dict.ignore_cols:\n",
    "                dfi.drop(col,axis=1,inplace=True)\n",
    "                dfv.drop(col,axis=1,inplace=True)\n",
    "                continue\n",
    "            if col in self.feat_dict.numeric_cols:\n",
    "                dfi[col] = self.feat_dict.feat_dict[col]\n",
    "            else:\n",
    "                dfi[col] = dfi[col].map(self.feat_dict.feat_dict[col])\n",
    "                dfv[col] = 1.\n",
    "\n",
    "        xi = dfi.values.tolist()\n",
    "        xv = dfv.values.tolist()\n",
    "\n",
    "        if has_label:\n",
    "            return xi,xv,y\n",
    "        else:\n",
    "            return xi,xv,ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "class DeepFM(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, feature_size, field_size,\n",
    "                 embedding_size=8, dropout_fm=[1.0, 1.0],\n",
    "                 deep_layers=[32, 32], dropout_deep=[0.5, 0.5, 0.5],\n",
    "                 deep_layer_activation=tf.nn.relu,\n",
    "                 epoch=10, batch_size=256,\n",
    "                 learning_rate=0.001, optimizer=\"adam\",\n",
    "                 batch_norm=0, batch_norm_decay=0.995,\n",
    "                 verbose=False, random_seed=2018,\n",
    "                 use_fm=True, use_deep=True,\n",
    "                 loss_type=\"logloss\", eval_metric=roc_auc_score,\n",
    "                 l2_reg=0.0, greater_is_better=True):\n",
    "        assert (use_fm or use_deep)\n",
    "        assert loss_type in [\"logloss\", \"mse\"], \\\n",
    "            \"loss_type can be either 'logloss' for classification task or 'mse' for regression task\"\n",
    "\n",
    "        self.feature_size = feature_size\n",
    "        self.field_size = field_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.dropout_fm = dropout_fm\n",
    "        self.deep_layers = deep_layers\n",
    "        self.dropout_dep = dropout_deep\n",
    "        self.deep_layers_activation = deep_layer_activation\n",
    "        self.use_fm = use_fm\n",
    "        self.use_deep = use_deep\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer_type = optimizer\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.batch_norm_decay = batch_norm_decay\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.random_seed = random_seed\n",
    "        self.loss_type = loss_type\n",
    "        self.eval_metric = eval_metric\n",
    "        self.greater_is_better = greater_is_better\n",
    "        self.train_result,self.valid_result = [],[]\n",
    "\n",
    "        self._init_graph()\n",
    "\n",
    "    def _init_graph(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            tf.set_random_seed(self.random_seed)\n",
    "\n",
    "            self.feat_index = tf.placeholder(tf.int32, shape=[None,None], name='feat_index')\n",
    "            self.feat_value = tf.placeholder(tf.float32, shape=[None,None], name='feat_value')\n",
    "\n",
    "            self.label = tf.placeholder(tf.float32,shape=[None,1],name='label')\n",
    "            self.dropout_keep_fm = tf.placeholder(tf.float32,shape=[None],name='dropout_keep_fm')\n",
    "            self.dropout_keep_deep = tf.placeholder(tf.float32,shape=[None],name='dropout_keep_deep')\n",
    "            self.train_phase = tf.placeholder(tf.bool,name='train_phase')\n",
    "\n",
    "            self.weights = self._initialize_weights()\n",
    "\n",
    "            # model\n",
    "            self.embeddings = tf.nn.embedding_lookup(self.weights['feature_embeddings'],self.feat_index) # N * F * K\n",
    "            feat_value = tf.reshape(self.feat_value,shape=[-1,self.field_size,1])\n",
    "            self.embeddings = tf.multiply(self.embeddings,feat_value)\n",
    "\n",
    "            # first order term\n",
    "            self.y_first_order = tf.nn.embedding_lookup(self.weights['feature_bias'],self.feat_index)\n",
    "            self.y_first_order = tf.reduce_sum(tf.multiply(self.y_first_order,feat_value),2)\n",
    "            self.y_first_order = tf.nn.dropout(self.y_first_order,self.dropout_keep_fm[0])\n",
    "\n",
    "            # second order term\n",
    "            # sum-square-part\n",
    "            self.summed_features_emb = tf.reduce_sum(self.embeddings,1) # None * k\n",
    "            self.summed_features_emb_square = tf.square(self.summed_features_emb) # None * K\n",
    "\n",
    "            # squre-sum-part\n",
    "            self.squared_features_emb = tf.square(self.embeddings)\n",
    "            self.squared_sum_features_emb = tf.reduce_sum(self.squared_features_emb, 1)  # None * K\n",
    "\n",
    "            # second order\n",
    "            self.y_second_order = 0.5 * tf.subtract(self.summed_features_emb_square,self.squared_sum_features_emb)\n",
    "            self.y_second_order = tf.nn.dropout(self.y_second_order,self.dropout_keep_fm[1])\n",
    "\n",
    "            # Deep component\n",
    "            self.y_deep = tf.reshape(self.embeddings,shape=[-1,self.field_size * self.embedding_size])\n",
    "            self.y_deep = tf.nn.dropout(self.y_deep,self.dropout_keep_deep[0])\n",
    "\n",
    "            for i in range(0,len(self.deep_layers)):\n",
    "                self.y_deep = tf.add(tf.matmul(self.y_deep,self.weights[\"layer_%d\" %i]), self.weights[\"bias_%d\"%i])\n",
    "                self.y_deep = self.deep_layers_activation(self.y_deep)\n",
    "                self.y_deep = tf.nn.dropout(self.y_deep,self.dropout_keep_deep[i+1])\n",
    "\n",
    "            # ----DeepFM---------\n",
    "            if self.use_fm and self.use_deep:\n",
    "                concat_input = tf.concat([self.y_first_order, self.y_second_order, self.y_deep], axis=1)\n",
    "            elif self.use_fm:\n",
    "                concat_input = tf.concat([self.y_first_order, self.y_second_order], axis=1)\n",
    "            elif self.use_deep:\n",
    "                concat_input = self.y_deep\n",
    "\n",
    "            self.out = tf.add(tf.matmul(concat_input,self.weights['concat_projection']),self.weights['concat_bias'])\n",
    "\n",
    "            # loss\n",
    "            if self.loss_type == \"logloss\":\n",
    "                self.out = tf.nn.sigmoid(self.out)\n",
    "                self.loss = tf.losses.log_loss(self.label, self.out)\n",
    "            elif self.loss_type == \"mse\":\n",
    "                self.loss = tf.nn.l2_loss(tf.subtract(self.label, self.out))\n",
    "            # l2 regularization on weights\n",
    "            if self.l2_reg > 0:\n",
    "                self.loss += tf.contrib.layers.l2_regularizer(\n",
    "                    self.l2_reg)(self.weights[\"concat_projection\"])\n",
    "                if self.use_deep:\n",
    "                    for i in range(len(self.deep_layers)):\n",
    "                        self.loss += tf.contrib.layers.l2_regularizer(\n",
    "                            self.l2_reg)(self.weights[\"layer_%d\" % i])\n",
    "\n",
    "            if self.optimizer_type == \"adam\":\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999,\n",
    "                                                        epsilon=1e-8).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"adagrad\":\n",
    "                self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate,\n",
    "                                                           initial_accumulator_value=1e-8).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"gd\":\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"momentum\":\n",
    "                self.optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=0.95).minimize(\n",
    "                    self.loss)\n",
    "\n",
    "            # init\n",
    "            self.saver = tf.train.Saver()\n",
    "            init = tf.global_variables_initializer()\n",
    "            self.sess = tf.Session()\n",
    "            self.sess.run(init)\n",
    "\n",
    "            # number of params\n",
    "            total_parameters = 0\n",
    "            for variable in self.weights.values():\n",
    "                shape = variable.get_shape()\n",
    "                variable_parameters = 1\n",
    "                for dim in shape:\n",
    "                    variable_parameters *= dim.value\n",
    "                total_parameters += variable_parameters\n",
    "            if self.verbose > 0:\n",
    "                print(\"#params: %d\" % total_parameters)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        weights = dict()\n",
    "\n",
    "        # embeddings\n",
    "        weights['feature_embeddings'] = tf.Variable(\n",
    "            tf.random_normal([self.feature_size,self.embedding_size],0.0,0.01),\n",
    "            name='feature_embeddings')\n",
    "        weights['feature_bias'] = tf.Variable(tf.random_normal([self.feature_size,1],0.0,1.0),name='feature_bias')\n",
    "\n",
    "        # deep layers\n",
    "        num_layer = len(self.deep_layers)\n",
    "        input_size = self.field_size * self.embedding_size\n",
    "        glorot = np.sqrt(2.0/(input_size + self.deep_layers[0]))\n",
    "\n",
    "        weights['layer_0'] = tf.Variable(\n",
    "            np.random.normal(loc=0,scale=glorot,size=(input_size,self.deep_layers[0])),dtype=np.float32\n",
    "        )\n",
    "        weights['bias_0'] = tf.Variable(\n",
    "            np.random.normal(loc=0,scale=glorot,size=(1,self.deep_layers[0])),dtype=np.float32\n",
    "        )\n",
    "\n",
    "        for i in range(1,num_layer):\n",
    "            glorot = np.sqrt(2.0 / (self.deep_layers[i - 1] + self.deep_layers[i]))\n",
    "            weights[\"layer_%d\" % i] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(self.deep_layers[i - 1], self.deep_layers[i])),\n",
    "                dtype=np.float32)  # layers[i-1] * layers[i]\n",
    "            weights[\"bias_%d\" % i] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[i])),\n",
    "                dtype=np.float32)  # 1 * layer[i]\n",
    "\n",
    "        # final concat projection layer\n",
    "        if self.use_fm and self.use_deep:\n",
    "            input_size = self.field_size + self.embedding_size + self.deep_layers[-1]\n",
    "        elif self.use_fm:\n",
    "            input_size = self.field_size + self.embedding_size\n",
    "        elif self.use_deep:\n",
    "            input_size = self.deep_layers[-1]\n",
    "\n",
    "        glorot = np.sqrt(2.0/(input_size + 1))\n",
    "        weights['concat_projection'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(input_size,1)),dtype=np.float32)\n",
    "        weights['concat_bias'] = tf.Variable(tf.constant(0.01),dtype=np.float32)\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def get_batch(self,Xi,Xv,y,batch_size,index):\n",
    "        start = index * batch_size\n",
    "        end = (index + 1) * batch_size\n",
    "        end = end if end < len(y) else len(y)\n",
    "        return Xi[start:end],Xv[start:end],[[y_] for y_ in y[start:end]]\n",
    "\n",
    "    # shuffle three lists simutaneously\n",
    "    def shuffle_in_unison_scary(self, a, b, c):\n",
    "        rng_state = np.random.get_state()\n",
    "        np.random.shuffle(a)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(b)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(c)\n",
    "\n",
    "    def evaluate(self, Xi, Xv, y):\n",
    "        \"\"\"\n",
    "        :param Xi: list of list of feature indices of each sample in the dataset\n",
    "        :param Xv: list of list of feature values of each sample in the dataset\n",
    "        :param y: label of each sample in the dataset\n",
    "        :return: metric of the evaluation\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(Xi, Xv)\n",
    "        return self.eval_metric(y, y_pred)\n",
    "\n",
    "    def predict(self, Xi, Xv):\n",
    "        \"\"\"\n",
    "        :param Xi: list of list of feature indices of each sample in the dataset\n",
    "        :param Xv: list of list of feature values of each sample in the dataset\n",
    "        :return: predicted probability of each sample\n",
    "        \"\"\"\n",
    "        # dummy y\n",
    "        dummy_y = [1] * len(Xi)\n",
    "        batch_index = 0\n",
    "        Xi_batch, Xv_batch, y_batch = self.get_batch(Xi, Xv, dummy_y, self.batch_size, batch_index)\n",
    "        y_pred = None\n",
    "        while len(Xi_batch) > 0:\n",
    "            num_batch = len(y_batch)\n",
    "            feed_dict = {self.feat_index: Xi_batch,\n",
    "                         self.feat_value: Xv_batch,\n",
    "                         self.label: y_batch,\n",
    "                         self.dropout_keep_fm: [1.0] * len(self.dropout_fm),\n",
    "                         self.dropout_keep_deep: [1.0] * len(self.dropout_dep),\n",
    "                         self.train_phase: False}\n",
    "            batch_out = self.sess.run(self.out, feed_dict=feed_dict)\n",
    "\n",
    "            if batch_index == 0:\n",
    "                y_pred = np.reshape(batch_out, (num_batch,))\n",
    "            else:\n",
    "                y_pred = np.concatenate((y_pred, np.reshape(batch_out, (num_batch,))))\n",
    "\n",
    "            batch_index += 1\n",
    "            Xi_batch, Xv_batch, y_batch = self.get_batch(Xi, Xv, dummy_y, self.batch_size, batch_index)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def fit_on_batch(self,Xi,Xv,y):\n",
    "        feed_dict = {self.feat_index:Xi,\n",
    "                     self.feat_value:Xv,\n",
    "                     self.label:y,\n",
    "                     self.dropout_keep_fm:self.dropout_fm,\n",
    "                     self.dropout_keep_deep:self.dropout_dep,\n",
    "                     self.train_phase:True}\n",
    "\n",
    "        loss,opt = self.sess.run([self.loss,self.optimizer],feed_dict=feed_dict)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def fit(self, Xi_train, Xv_train, y_train,\n",
    "            Xi_valid=None, Xv_valid=None, y_valid=None,\n",
    "            early_stopping=False, refit=False):\n",
    "        \"\"\"\n",
    "        :param Xi_train: [[ind1_1, ind1_2, ...], [ind2_1, ind2_2, ...], ..., [indi_1, indi_2, ..., indi_j, ...], ...]\n",
    "                         indi_j is the feature index of feature field j of sample i in the training set\n",
    "        :param Xv_train: [[val1_1, val1_2, ...], [val2_1, val2_2, ...], ..., [vali_1, vali_2, ..., vali_j, ...], ...]\n",
    "                         vali_j is the feature value of feature field j of sample i in the training set\n",
    "                         vali_j can be either binary (1/0, for binary/categorical features) or float (e.g., 10.24, for numerical features)\n",
    "        :param y_train: label of each sample in the training set\n",
    "        :param Xi_valid: list of list of feature indices of each sample in the validation set\n",
    "        :param Xv_valid: list of list of feature values of each sample in the validation set\n",
    "        :param y_valid: label of each sample in the validation set\n",
    "        :param early_stopping: perform early stopping or not\n",
    "        :param refit: refit the model on the train+valid dataset or not\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        has_valid = Xv_valid is not None\n",
    "        for epoch in range(self.epoch):\n",
    "            t1 = time()\n",
    "            self.shuffle_in_unison_scary(Xi_train, Xv_train, y_train)\n",
    "            total_batch = int(len(y_train) / self.batch_size)\n",
    "            for i in range(total_batch):\n",
    "                Xi_batch, Xv_batch, y_batch = self.get_batch(Xi_train, Xv_train, y_train, self.batch_size, i)\n",
    "                self.fit_on_batch(Xi_batch, Xv_batch, y_batch)\n",
    "\n",
    "            # evaluate training and validation datasets\n",
    "            train_result = self.evaluate(Xi_train, Xv_train, y_train)\n",
    "            self.train_result.append(train_result)\n",
    "            if has_valid:\n",
    "                valid_result = self.evaluate(Xi_valid, Xv_valid, y_valid)\n",
    "                self.valid_result.append(valid_result)\n",
    "            if self.verbose > 0 and epoch % self.verbose == 0:\n",
    "                if has_valid:\n",
    "                    print(\"[%d] train-result=%.4f, valid-result=%.4f [%.1f s]\"\n",
    "                        % (epoch + 1, train_result, valid_result, time() - t1))\n",
    "                else:\n",
    "                    print(\"[%d] train-result=%.4f [%.1f s]\"\n",
    "                        % (epoch + 1, train_result, time() - t1))\n",
    "            if has_valid and early_stopping and self.training_termination(self.valid_result):\n",
    "                break\n",
    "\n",
    "        # fit a few more epoch on train+valid until result reaches the best_train_score\n",
    "        if has_valid and refit:\n",
    "            if self.greater_is_better:\n",
    "                best_valid_score = max(self.valid_result)\n",
    "            else:\n",
    "                best_valid_score = min(self.valid_result)\n",
    "            best_epoch = self.valid_result.index(best_valid_score)\n",
    "            best_train_score = self.train_result[best_epoch]\n",
    "            Xi_train = Xi_train + Xi_valid\n",
    "            Xv_train = Xv_train + Xv_valid\n",
    "            y_train = y_train + y_valid\n",
    "            for epoch in range(100):\n",
    "                self.shuffle_in_unison_scary(Xi_train, Xv_train, y_train)\n",
    "                total_batch = int(len(y_train) / self.batch_size)\n",
    "                for i in range(total_batch):\n",
    "                    Xi_batch, Xv_batch, y_batch = self.get_batch(Xi_train, Xv_train, y_train,\n",
    "                                                                self.batch_size, i)\n",
    "                    self.fit_on_batch(Xi_batch, Xv_batch, y_batch)\n",
    "                # check\n",
    "                train_result = self.evaluate(Xi_train, Xv_train, y_train)\n",
    "                if abs(train_result - best_train_score) < 0.001 or \\\n",
    "                    (self.greater_is_better and train_result > best_train_score) or \\\n",
    "                    ((not self.greater_is_better) and train_result < best_train_score):\n",
    "                    break\n",
    "\n",
    "    def training_termination(self, valid_result):\n",
    "        if len(valid_result) > 5:\n",
    "            if self.greater_is_better:\n",
    "                if valid_result[-1] < valid_result[-2] and \\\n",
    "                    valid_result[-2] < valid_result[-3] and \\\n",
    "                    valid_result[-3] < valid_result[-4] and \\\n",
    "                    valid_result[-4] < valid_result[-5]:\n",
    "                    return True\n",
    "            else:\n",
    "                if valid_result[-1] > valid_result[-2] and \\\n",
    "                    valid_result[-2] > valid_result[-3] and \\\n",
    "                    valid_result[-3] > valid_result[-4] and \\\n",
    "                    valid_result[-4] > valid_result[-5]:\n",
    "                    return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "start = time.clock()\n",
    "def load_data():\n",
    "    dfTrain = pd.read_csv(TRAIN_FILE)\n",
    "    dfTest = pd.read_csv(TEST_FILE)\n",
    "    \n",
    "    def preprocess(df):\n",
    "        cols = [c for c in df.columns if c not in [\"card_id\", \"target\"]]\n",
    "        return df\n",
    "    \n",
    "    dfTrain = preprocess(dfTrain)\n",
    "    dfTest = preprocess(dfTest)\n",
    "    \n",
    "    cols = [c for c in dfTrain.columns if c not in [\"card_id\", \"target\"]]\n",
    "    cols = [c for c in cols if (not c in IGNORE_COLS)]\n",
    "    \n",
    "    X_train = dfTrain[cols].values\n",
    "    y_train = dfTrain[\"target\"].values\n",
    "    \n",
    "    X_test = dfTest[cols].values\n",
    "    ids_test = dfTest[\"card_id\"].values\n",
    "    \n",
    "    cat_features_indices = [i for i, c in enumerate(cols) if c in CATEGORICAL_COLS]\n",
    "    \n",
    "    return dfTrain, dfTest, X_train, y_train, X_test, ids_test, cat_features_indices\n",
    "\n",
    "\n",
    "def run_base_model_dfm(dfTrain, dfTest, folds, dfm_params):\n",
    "    fd = FeatureDictionary(dfTrain=dfTrain, dfTest=dfTest, numeric_cols=NUMERIC_COLS,\n",
    "                           ignore_cols=IGNORE_COLS)\n",
    "    data_parser = DataParser(feat_dict=fd)\n",
    "    \n",
    "    Xi_train, Xv_train, y_train = data_parser.parse(df=dfTrain, has_label=True)\n",
    "    Xi_test, Xv_test, ids_test = data_parser.parse(df=dfTest)\n",
    "    print(dfTrain.dtypes)\n",
    "    \n",
    "    dfm_params[\"feature_size\"] = fd.feat_dim\n",
    "    dfm_parmas[\"field_size\"] = len(Xi_train[0])\n",
    "    \n",
    "    y_train_meta = np.zeros((dfTrain.shape[0], 1), dtype=float)\n",
    "    y_test_meta = np.zeros((dfTest.shape[0], 1), dtype=float)\n",
    "    \n",
    "    obtain = lambda x,l:[x[i] for i in l]\n",
    "    \n",
    "    gini_result_cv = np.zeros(len(folds), dtype=float)\n",
    "    gini_result_epoch_train = np.zeros((len(flods), dfm_params[\"epoch\"]), dtype=float)\n",
    "    gini_result_epoch_valid = np.zeros((len(flods), dfm_params[\"epoch\"]), dtype=float)\n",
    "    \n",
    "    for i, (train_idx, valid_idx) in enumerate(folds):\n",
    "        Xi_train_, Xv_train_, y_train_ = obtain(Xi_train, train_idx), obtain(Xv_train, train_idx), \\\n",
    "                                         obtain(y_train, train_idx)\n",
    "        Xi_valid_, Xv_valid_, y_valid_ = obtain(Xi_train, valid_idx), obtain(Xv_train, valid_idx), \\\n",
    "                                         obtain(y_train, valid_idx)\n",
    "        dfm = DeepFM(**dfm_params)\n",
    "        dfm.fit(Xi_train_, Xv_train_, y_train_, Xi_valid_, Xv_valid_, y_valid_)\n",
    "        y_train_meta[valid_idx, 0] = dfm.predict(X9_valid_, Xv_valid_)\n",
    "        y_test_meta[:, 0] += dfm.predict(Xi_test, Xv_test)\n",
    "        gini_results_epoch_train[i] = dfm.train_result\n",
    "        gini_results_epoch_valid[i] = dfm.valid_result\n",
    "        \n",
    "    y_test_meta /= float(len(folds))\n",
    "    \n",
    "    if dfm_params[\"use_fm\"] and dfm_param[\"use_deep\"]:\n",
    "        clf_str = \"DeepFM\"\n",
    "    elif dfm_params[\"use_fm\"]:\n",
    "        clf_str = \"FM\"\n",
    "    elif dfm_params[\"use_deep\"]:\n",
    "        clf_str = \"DNN\"\n",
    "    print(\"%s: %.5f (%.5f)\" % (clf_str, gini_results_cv.mean(), gini_results_cv.std()))\n",
    "    filename = \"%s_Mean%.5f_Std%.5f.csv\" % (clf_str, gini_results_cv.mean(), gini_results_cv.std())\n",
    "    _make_submission(ids_test, y_test_meta, filename)\n",
    "    \n",
    "    aucplot(y_valid_, y_train_meta[valid_idx, 0], clf_str)\n",
    "    \n",
    "    return y_train_meta, y_test_meta\n",
    "\n",
    "\n",
    "def _make_submission(ids, y_pred, filename=\"submission.csv\"):\n",
    "    pd.DataFrame({\"card_id\": ids, \"target\": y_pred.flatten()}).to_csv(os.path.join(SUB_FILE, filename),\n",
    "                                                                      index=False, float_format=\"%.5f\")\n",
    "    \n",
    "def aucplot(y, y_predprobs, model_name):\n",
    "    fpr, tpr, threshold = roc_curve(y, y_predprobs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color=\"darkorange\", lw=lw, label=\"ROC curve (area=%0.5f)\" % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(model_name + \"ROC\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    if os.path.exists(\"../fig/\") == False:\n",
    "        os.makedirs(\"../fig/\")\n",
    "    plt.savefig(\"../fig/\" + model_name + \"_roc_\" + str(int(time.time())) + \".png\")\n",
    "    plt.show()\n",
    "\n",
    "dfm_params = {\n",
    "    \"use_fm\":True,\n",
    "    \"use_deep\":True,\n",
    "    \"embedding_size\":8,\n",
    "    \"dropout_fm\":[1.0,1.0],\n",
    "    \"deep_layers\":[32,32],\n",
    "    \"dropout_deep\":[0.5,0.5,0.5],\n",
    "    \"deep_layer_activation\":tf.nn.relu,\n",
    "    \"epoch\":30,\n",
    "    \"batch_size\":1024,\n",
    "    \"learning_rate\":0.001,\n",
    "    \"optimizer\":\"adam\",\n",
    "    \"batch_norm\":1,\n",
    "    \"batch_norm_decay\":0.995,\n",
    "    \"l2_reg\":0.01,\n",
    "    \"verbose\":True,\n",
    "    \"random_seed\":RANDOM_SEED\n",
    "}\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dfTrain, dfTest, X_train, X_test, ids_test, cat_features_indices = load_data()\n",
    "    folds = list(StratifiedKFold(n_splits=NUM_SPLITS, shuffle=True,\n",
    "                                random_state=RANDOM_SEED).split(X_train, y_train))\n",
    "    y_train_dfm, y_test_dfm = run_base_model_dfm(dfTrain, dfTest, folds, dfm_params)\n",
    "    \n",
    "#     fm_params = dfm_params.copy()\n",
    "#     fm_params[\"use_deep\"] = False\n",
    "#     y_train_fm, y_test_fm = run_base_model_dfm(dfTrain, dfTest, folds, fm_params)\n",
    "    \n",
    "#     dnn_params = dfm_params.copy()\n",
    "#     dnn_params[\"use_fm\"] = False\n",
    "#     y_train_dnn, y_test_dnn = run_base_model_dfm(dfTrain, dfTest, folds, dnn_params)\n",
    "    \n",
    "    end = time.clock()\n",
    "    print(\"time: %s seconds\" % (end-start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
